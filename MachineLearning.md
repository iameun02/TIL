# ML ê¸°ì´ˆ
> Machine Learning (ì¸ê³µì§€ëŠ¥) ê¸°ì´ˆí•™ìŠµ   

<br>

## Jupyter Notebook ì‚¬ìš© tip
1. shift + tab + tab : í•¨ìˆ˜ì„¤ëª…
2. Command + ì¢Œìš° >> ë¬¸ì¥ ëìœ¼ë¡œ ì´ë™ / alt + ì¢Œìš° : ë¬¸ë‹¨ ëìœ¼ë¡œ ì´ë™
3. x = í–‰ì‚­ì œ
4. z = í–‰ì‚­ì œ ì·¨ì†Œ


<br><br>

## Numpy/ Pandas ì½”ë“œ

```python
    [numpy]
 1. í•¨ìˆ˜ ë° ë©”ì„œë“œ
    type(): ìë£Œí˜• ì •ë³´ (list, array ë“±) 
    .dtype : ë°ì´í„° íƒ€ì…ì •ë³´ (int, float ë“±)
    df.select_dtypes("object") : df ë°ì´í„° ì¤‘ 'object'íƒ€ì…ì˜ ë°ì´í„°ë“¤ë§Œ ì¶”ì¶œ
    .iloc[:, df.dtypes =='int64'] : ìƒë™
    .reset_index : ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    .value_counts()
    .sort_values(by=[''])

 2. íœì‹œ ì¸ë±ì‹± (ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì¤˜ì•¼í•¨)
    df.iloc[[0,3,5,7],[3,5,7]]
    df.loc[0:3,['name','age','salary']] 
     (loc: 0-3 ë˜í•œ ìˆ«ìí˜•ì´ ì•„ë‹Œ ë¬¸ìì—´ë¡œ ì¸ë±ì‹± ëœ ê²ƒì´ë‹¤.)

 3. ë¶€ìš¸ ì¸ë±ì‹±
    df_hk.loc[df_hk['age']>30, :] #í–‰ì„ íƒì„ ì¡°ê±´ë¬¸ìœ¼ë¡œ ì¤„ìˆ˜ë„ ìˆë‹¤. 
 
 4. ì–´ë– í•œ ê°’ì´ë¼ë„ ë¬¸ìë¡œ ì²˜ë¦¬í• ë•Œ ë”°ì˜´í‘œ 3ê°œ
    '''abc'''
    """abc""" 
 
 5. a = [1,2,3], b = [4,5,6] ì¼ ë•Œ, 
    a + b = [1,2,3,4,5,6] ìœ¼ë¡œ ì¶œë ¥ëœë‹¤.
    í–‰ë ¬ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´ì„  1ì°¨ì› ë°°ì—´ë¡œ ë°”ê¿”ì¤˜ì•¼í•œë‹¤. 
    a = np.array[1,2,3] , b= array[4,5,6]
    a + b = [5,6,9]

 6. íŠœí”Œì€ í•˜ë‚˜ì˜ ì›ì†Œì¼ë•Œë„ ë’¤ì— ,ë¥¼ ê¼­! ë¶™ì—¬ì¤˜ì•¼ í•¨ (3,) ì•„ë‹ˆë©´ ìˆ«ìí˜•ìœ¼ë¡œ ì¸ì‹ë¨ (3)

 7. print ì™€ returnì˜ ì°¨ì´ì   
    returnì€ ê²°ê³¼ê°’ì„ ë˜ ê°’ìœ¼ë¡œ ë°›ì•„ì„œ ë‹¤ë¥¸ ê³³ì—ì„œ í™œìš© ê°€ëŠ¥ , í”„ë¦°íŠ¸ëŠ” ë‹¨ìˆœ ì¶œë ¥ë§Œ

 8. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   [terminal]  
      pip install tensorflow
   [jupyter notebook] \
      !pip install tensorflow
   ë˜ëŠ”
      import os
      os.system('pip install tensorflow')

 9. ë¶ˆë¦°ì¸ë±ìŠ¤ëŠ” locë§Œ ì‚¬ìš©ê°€ëŠ¥ / ilocì€ ë¶ˆê°€
   result = df_food.drop(columns=['state'])
   len(result[(result['cook_time']>0) & result['prep_time']>0])

 10. ë²”ìœ„ë¡œ ë°°ì—´ ìƒì„±
    range (ë¦¬ìŠ¤íŠ¸)
    np.arange(ë°°ì—´)
    
 11. nê°œ column ì— ëŒ€í•œ aggregation
         df_hk[['age','salary']].agg(['mean','sum','count'])
     nê°œ column ì— ëŒ€í•œ nê°œ aggregation_ê°ê° ìš”ì•½ì •ë³´ë¥¼ ë‹¤ë¥´ê²Œ ì¶œë ¥í•˜ê³  ì‹¶ì„ë•Œ ë”•ì…”ë„ˆë¦¬ ì´ìš©
     ì´ë•Œ 1ê°œ columnì— 2ê°œ ì´ìƒ aggregationì„ ì ìš©í•˜ë©´ ë§ˆì§€ë§‰ aggregationì´ ì ìš© (age : maxì ìš©ë¨)
         df_hk[['age','salary']].agg({'age':'mean', 'age':'max', 'salary':'max'})
     ë‘ê°œë¥¼ ë‹¤ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ ë°©ë²• í™œìš© (ê°ê° ì—´ì„ ì •ì˜í•˜ì—¬ ìƒì„±)
         df_hk[['age','salary']].agg(age_mean = ('age','mean'), age_max = ('age','max'), sal_cnt = ('salary','count'), sal_median = ('salary','median'))


 12. MLëŒë¦´ë•Œ í•˜ë‚˜ë¡œë„ ê²°ì¸¡ì¹˜ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ë‚˜ê²Œëœë‹¤. Merge_keyê°’ìœ¼ë¡œ ì¡°ì¸ì‹œ ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ”ì§€ í™•ì¸ì´ í•„ìš”

 13. replace ë˜ëŠ” map #dictí™œìš© 
     df_hk['gender'].replace({'F':'Female' ,'M':'Man'})

 14. Lambda
     df_hk['jumin7'].apply(lambda x : '19'+ x[0:6])

 15. ë“±ê¸‰í™” : np.where ë˜ëŠ” cut ì´ìš©

      #np.where (ì—‘ì…€ ifë¬¸ê³¼ ë™ì¼)
      np.where(df_hk['age'] < 30,'30ë¯¸ë§Œ',
            np.where(df_hk['age']<=40, '40ëŒ€','50ëŒ€ì´ìƒ'))[:10]
      # cut
      pd.cut(x = df_hk['age'], bins = [0,20,30,40,100], labels = ['10ëŒ€', '20ëŒ€', '30ëŒ€', '40ëŒ€ì´ìƒ'], right= True)
      * right = True : ì˜¤ë¥¸ìª½ ê°’ì„ í•´ë‹¹êµ¬ê°„ì— í¬í•¨í•˜ëŠ”ê²Œ T, ë‹¤ìŒ êµ¬ê°„ì— í¬í•¨í•˜ëŠ”ê²Œ F 
                       ì¦‰, ì´í•˜ì¸ì§€ ë¯¸ë§Œì¸ì§€ 
      
      binsìˆ˜ (êµ¬ê°„ì´ë‹¤ë³´ë‹ˆ) > labelìˆ˜

 16. datetime(ë‚ ì§œ)í˜•ìœ¼ë¡œ ë³€í™˜ 2ê°€ì§€ ë°©ë²•
     pd.to_datetime(df_hk['jumin7'], format = '%y%m%d-%w')
     .astype('datetime64')

     í™œìš©
      # datetime í™œìš©
      df_hk['birthday'] = df_hk['jumin7'].apply(lambda x : '19'+ x[0:6]).astype('datetime64')
      df_hk['birthday'].dt.year
      df_hk['birthday'].dt.month
      df_hk['birthday'].dt.day
      df_hk['birthday'].dt.weekday #ì›”ìš”ì¼ì€ 0
      df_hk['birthday'].dt.day_name() #day_nameì€ ()í•„ìš”
 17. Array ìƒì„±
     np.array([1,2,3])
     np.zeros((3,3))

 18. .tolist()
```
<br><br>

## Series ì™€ DataFrame
[Series] :  Pandasì˜ 1ì°¨ì›
 1) í•¨ìˆ˜ ë° ë©”ì„œë“œ
      A = pd.Series([1,2,3])
      A.values
      A.index
      A.idxmax(ìµœëŒ€ê°’ì˜ ìœ„ì¹˜)
      A.isin([1,2])

[DataFrame] : 1ì°¨ì› Seriesë¥¼ ëª¨ì•„ë†“ì€ ê²ƒ Pandasì˜ 2ì°¨ì› 
 1) í•¨ìˆ˜ ë° ë©”ì„œë“œ
   Crosstab  
    - ë‘ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì‚´í´ë³¼ë•Œ ì‚¬ìš©í•˜ëŠ” pandas í•¨ìˆ˜ (ë¹ˆë„í‘œ)
    - normalize ì¸ìì— True,1,0ê°’ì„ í• ë‹¹í•˜ì—¬ ê°’ì„ ì •ê·œí™”í• ìˆ˜ìˆìŒ
      pd.crosstab(df['aa'], df['bb'])


      pd.DataFrame([1,2,3], [4,5,6]], columns = ['A', 'B', 'C'])
      pd.DataFrame({'A':[1,4], 'B':[2,5], 'C':[3,6]})
      pd.DataFrame(np.arange(6).reshape(2,3) +1 , columns=['A','B','C'])

<br><br>

## í†µê³„
 1. mode
    ```python
    from scipy.stats import mode
    mode(array) #mode : ìµœë¹ˆê°’ (ë¹ˆë„ê°’ì´ ëª¨ë‘ ë™ì¼ í•  ë•ŒëŠ” ì²«ë²ˆì§¸ ê°’ ë¦¬í„´), count : ë¹ˆë„ìˆ˜)
    ```


 2. [ì¡°ê±´ë¶€í™•ë¥ ]ì€ crosstabì„ í™œìš©í•˜ì—¬ í‘¼ë‹¤.
    pd.crosstab(df['a'], df['b'])

 3. [ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬ ì¦ëª…]
   ```python
   import matplotlib.pyplot as plt
   import numpy as np
   import random


   avg_values = []
   for i in range (1,10000): # íšŸìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ì •ê·œë¶„í¬ë¡œ ë³€í™”
   random_sample = random.sample(range(1, 1000),100)
   x = np.mean(random_sample)
   avg_values.append(x)

   plt.hist(avg_values, bins = 100)
   plt.show()
   ```

<br><br>

## í‘œë³¸ì¶”ì¶œ

1. ë‹¨ìˆœì„ì˜ì¶”ì¶œ
2. ì¸µí™”í‘œë³¸ì¶”ì¶œ (ì˜ë£Œ-ì½”í˜¸íŠ¸ë¶„ì„)
3. ê³„í†µì¶”ì¶œ
    - ì²«í‘œë³¸ì„ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•˜ê³  í‘œì§‘ê°„ê²© kë§Œí¼ ë–¨ì–´ì§„ ê³³ì—ì„œ ë°ì´í„° ì¶”ì¶œ
4. êµ°ì§‘ì¶”ì¶œ

```Basic Code_ Random Sampling``` 
  > ìƒ˜í”Œ ì¶”ì¶œì€ ë‹¨ìˆœì„ì˜ ì¶”ì¶œì´ ê¸°ë³¸ì´ì§€ë§Œ, groupby ë¥¼ ì‚¬ìš©í•˜ë©´ ì¸µí™” í‘œë³¸ì¶”ì¶œ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ]
Sklearn - train_test_split
df_train, df_test = train_test_split(df, test_size = 0.3, random_state = 42)

1)  pandas - sample() <br>
   n= í‘œë³¸ê°œìˆ˜ frac= ë¹„ìœ¨, random_state = í‘œë³¸ì¶”ì¶œê²°ê³¼ë¥¼ ê³ ì •
   Replace : ë³µì›ì¶”ì¶œ ì—¬ë¶€, weights : ê°€ì¤‘ì¹˜
   íŒŒë¼ë¯¸í„° n ê³¼ fracì€ ë™ì‹œì‚¬ìš©ì€ ë¶ˆê°€ ```(fracì€ ë¹„ìœ¨ë¡œ ì¶”ì¶œ 0.005ë“±)``` <br>
      ```python
      df.groupby('season').sample(n=2, random_state= 42) >> ì¸µí™”ì¶”ì¶œê¸°ëŠ¥
      ```


2) scikit-learn <br>
   ```python
      from sklearn.model_selection import train_test_split

      df_train, df_test = train_test_split(df, test_size = 0.3, random_state = 42)
   ```
   <br><br>


```í‘œë³¸ ì¶”ì¶œ ë°©ë²•ë³„ ì½”ë“œ```

* ###   Random sampling
   ```python
   from sklearn.model_selection import train_test_split
   x_train, x_test = train_test_split(df_hk, test_size =0.3, random_state=42)
   x_train[:5]
   ```

* ### Sytematic sampling
  ```python
   df_idx = df_hk.reset_index()

   x_train = df_idx[(df_idx['index'] % 5 != 0)]
   x_test = df_idx[(df_idx['index'] % 5 == 0)]
   ```

* ### Cluster sampling
   ```python
   df_cluster = df_hk[df_hk['company'] =='A']
   df_cluster
   ```
* ### Stratified Random Sampling
   ```python
   df_hk['company'].value_counts()

   pd.crosstab(df_hk['company'], df_hk['gender'])
   ```
<br><br>

## ë°ì´í„° ì „ì²˜ë¦¬

   <b>í•¨ìˆ˜ ë°  ë©”ì„œë“œ</b>
   ```python
    1.  .isna(), .isnull(), .notna(), .notnull() 
      # null ê³¼ NaNê³¼ naì€ ë™ì¼í•˜ë‹¤ê³  ë´ë„ ë¬´ê´€

        df.notna().sum(axis = 1)

    2. .fillna()
       df = df.fillna(value = {'Sepal_Width':df['Sepal_Width'].mean()}) 
       ë˜ëŠ”
       df['Sepal_Width'] = df['Sepal_Width'].fillna(df['Sepal_Width'].mean())
       round(df.var(),3)

    3. .select_dtypes()
       df.select_dtypes('float64').isnull().sum().sum()


    4. Outlier (í‰ê· + 1.5 í‘œì¤€í¸ì°¨ì¼ë•Œ)
       mean = df3['Sepal.Length'].mean()
       std = df3['Sepal.Length'].std()
       len(df[(df3['Sepal.Length'] > mean+ (1.5*std))|(df3['Sepal.Length'] < mean- (1.5*std))])

    5. ì¡°ê±´ ì¶”ì¶œ numpy - where()í•¨ìˆ˜
       a = np.where(df['Sepal_Length'] >= 3)

       np.where(df_hk['age'] < 30,'30ë¯¸ë§Œ',
            np.where(df_hk['age']<=40, '40ëŒ€','50ëŒ€ì´ìƒ'))[:10]

       #ì¡°ê±´ìœ¼ë¡œ ë²”ì£¼ ë‚˜ëˆŒë•Œ npwhere, ì¡°ê±´ìœ¼ë¡œ ì¶”ì¶œì‹œ loc 
      
    6. ë³€ìˆ˜ëª… ë°”ê¾¸ê¸° 
       [íŠ¹ì • ì»¬ëŸ¼ ë³€ê²½ : pandas - .rename() ì‚¬ìš©]
       df4 = df4.rename(columns = {'is_setosa':'is_seto'})
       Dataë¡œ ì ‘ê·¼í›„ íŒŒë¼ë¯¸í„°ì—ì„œ ì»¬ëŸ¼ ì§€ì •

       [ë°ì´í„° ì „ì²´ ì¼ê´„ ë³€ê²½ : pandas - .columns ì‚¬ìš©]
       df4.columns = ['name', 'gender', 'height', 'age', 'blood_type', 'company']

    7. .astype('typeëª…')

    8. .apply(lambda x : x, )

    9. .str.slice(0:4)
       ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹±

    10. to_datetime(a)
    
      a= df5[df5['casual']>25]['datetime'] #í•„í„°ë§
      b = pd.to_datetime(a)
      b.dt.date.nunique()
      bike_time = pd.to_datetime(bike['date time'][:3])
      bike_time.dt.year
      bike_time.dt.month
      bike_time.dt.hour
      bike_time.dt.date

    11. ë”ë¯¸ë³€ìˆ˜í™”
      pd.get_dummies (data, columns= [''], drop_first =True) #get_dummiesëŠ” ê°€ë³€ìˆ˜ ëŒ€ìƒì€ ì—†ì•°
      #drop_firstë¥¼ ì‚¬ìš©í•˜ì§€ì•Šê³  ê°€ë³€ìˆ˜ë¥¼ ëª¨ë‘ í™œìš©í•  ê²½ìš°, ì™„ì „ê³µì„ ì„± ë“± ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œ ë°œìƒ
    
    12. reset_index(drop = True) : ê¸°ì¡´ì˜ ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” ë©”ì„œë“œ + ë™ì‹œì— seriesë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• ë„ í•¨ 
    (drop = True : ê¸°ì¡´ ì¸ë±ìŠ¤ ì œê±°)

    13. .set_index('ì»¬ëŸ¼ëª…') : íŠ¹ì •ë³€ìˆ˜ë¥¼ ì¸ë±ìŠ¤ë¡œ ì§€ì •í•  ê²½ìš° ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ, ë°ì´í„° ë³‘í•© ë˜ëŠ” ì‹œê³„ì—´ ë¶„í•´ì—ì„œ ì—°ì‚°ì„ ìœ„í•´ í™œìš©

    14. ìµœëŒ€ê°’ì„ ê°€ì§„ í–‰ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬í• ë•Œ idxmax()

      [ì˜ˆì œ1]
      df5['datetime']=pd.to_datetime(df5['datetime'])
      df5['hour']= df5['datetime'].dt.hour
      df5.groupby('hour')['registered'].mean().idxmax()

      [ì˜ˆì œ2]
      a= bike[(bike['season']==2)].groupby(bike['datetime'].dt.hour)['registered'].mean()
      b= bike[(bike['season']==4)].groupby(bike['datetime'].dt.hour)['registered'].mean()
      abs(a-b).idxmax()

    15. Bindingì—°ì‚° (í–‰ ë˜ëŠ” ì—´ ì—°ì‚°ê°€ëŠ¥) : pandas- concat() 

      pd.concat([bike1, bike2.reset_index(drop = True)], axis = 1).reset_index(drop=True)

    16. Joinì—°ì‚° : pandas- merge()

      pd.merge(left = df_a, right = df_b , left_on = 'member', right_on = 'name',
            how = 'inner')
    17. crosstab() : wide form >> clustering(êµ°ì§‘í™”) ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ë¨

      pd.crosstab(dia['cut'],dia['clarity'], normalize =True).round(2)
      
      # normalize =True : ë°”ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ í™•ì¸ì´ ê°€ëŠ¥
      # normalize ëŠ” True/ False ì™¸ì— 1,0ë„ ìˆë‹¤
      # normalize = 1: ì—´ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” / 0 : í–‰ê¸°ì¤€

      #cutê³¼ clarity ë³„ë¡œ ê°€ê²©ì˜ í‰ê· 
      pd.crosstab(dia['cut'],dia['clarity'], values =dia['price'], aggfunc = pd.Series.mean)
      
      #ìœ„ì˜ ë‚´ìš©ì˜ long form ë²„ì ¼ (ê²°ê³¼ê°’ì€ ë™ì¼)
      dia.groupby(['cut', 'clarity'])['price'].mean().reset_index()
  
  18. melt()
   wide form ë°ì´í„°í”„ë ˆì„ì„ Long Formìœ¼ë¡œ ìë£Œêµ¬ì¡°ë¥¼ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ, id_vars ì¸ìì— ê¸°ì¤€ì´ ë˜ëŠ” ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì²˜ë¦¬ <br>
   
   í¬ë¡œìŠ¤íƒ­ì—ì„œ melt ì‚¬ìš© ì‹œì—ëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤í™” ë˜ì–´ìˆëŠ” ì—´ì„ reset_index()ë¥¼ í†µí•´ ì—´ ë¨¼ì € ë˜ëŒë ¤ ì£¼ê³  melt í•¨ìˆ˜ ì‚¬ìš©

   elec_melt = elec.melt(id_vars= ['YEAR','MONTH','DAY'])
   elec_melt

 19. pivot()
   Long form ë°ì´í„° í”„ë ˆì„ì„ wide formìœ¼ë¡œ ìë£Œêµ¬ì¡°ë¥¼ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
   Index / columns/ values ì¸ìì— ê°ê° ëŒ€ìƒë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì¶œë ¥ë°ì´í„° êµ¬ì¡°ë¥¼ ê²°ì • <br>
   crosstabì˜ value + aggregate functionì™€ ê¸°ëŠ¥ì´ ë™ì¼í•˜ì§€ë§Œ, í”¼ë²—ì´ ë” ë§ì´ ì“°ì„ <br>
   ë˜í•œ êµ°ì§‘ë¶„ì„ ì‹¤ì‹œ ì „ ë°ì´í„° ì „ì²˜ë¦¬ì— ì£¼ë¡œ í™œìš©ë¨
   (pivot()ì€ pv ì‹¤í–‰ë§Œ, pivot_table()ì€ ìš”ì•½ ì—°ì‚°ê¹Œì§€)
 20. between(2005,2007) : 2005~2007 ê¹Œì§€ 
 
 21. nlargest(5) í°ìˆœì„œëŒ€ë¡œ ìƒì‰¬ 5ê°œ ì¶”ì¶œ 

 22. drop()
   
   # drop ì€ ë°ì´í„° ë‹¨ìœ„ë¡œ ì ‘ê·¼ 
   # 1. dropnaì€ subset ì´ìš©
     df_hk_na= df_hk_na.dropna(subset ='age')

   # 2. drop(row index ì‚¬ìš©)ìœ¼ë¡œ ì‚­ì œ
     df_hk.drop(index = df_hk[(df_hk['expenditure'] < min) | (df_hk['expenditure'] > max)].index
  
   # 2-1(ì¤‘ìš”!!). drop ëŒ€ì‹  ë°˜ëŒ€ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§ ê°€ëŠ¥ 
     df_hk[(df_hk['expenditure'] >= min) & (df_hk['expenditure'] <= max)]
     
     [ì°¸ê³ ] (í–‰ì¶”ì¶œì‹œ 'NOT'ì€ ~ ë¡œ ì‚¬ìš© ê°€ëŠ¥)
     df_ds = df_ds[(~df_ds['last_new_job'].isin(['>4', 'never']))]

   # 3. ê·¸ì™¸ columns ì‚¬ìš©í•˜ì—¬ ì‚­ì œ ê°€ëŠ¥

   23. .pow(2) : ì§€ìˆ˜ì—°ì‚° (2ì¼ë•ŒëŠ” ì œê³±)
        ** ì™€ ë™ì¼ (**0.5 ëŠ” ë£¨íŠ¸ê°€ ëœë‹¤)

   ```

 
     

  
   <br><br>

 ## ë°ì´í„° ì‹œê°í™”

 ```import matplotlib.pyplot as plt``` <br>
 ```import seaborn as sns ```<br><br>

 ## #1. histogram 

<b>[matplotlib]</b> <br>
ì†ì„±ê°’ ì°¸ê³  : https://matplotlib.org/stable/api/markers_api.html
```python
figure = plt.figure(figsize=(5, 4), facecolor ='pink')
plt.plot([0,1,2,3],[1,4,9,12], color = 'red', marker = 'o', markersize = 3,
         linestyle ='dashed', linewidth = 0.5)
plt.plot([0,1,2,3],[10,20,30,40], color = 'gray', marker = '', markersize = 8,
         linestyle ='solid', linewidth = 0.5)

plt.xlabel('x_label')
plt.ylabel('y_label')
plt.title('matplotlib chart')


plt.show()
```
<br><br>
<b>[seaborn]</b> <br>

* seabornì˜ histogramì€ histplotê³¼ displotì´ ëŒ€í‘œì ì´ë©° histplotì€ axesë ˆë²¨, displotì€ figureë ˆë²¨ì„
```python
# seaborn histogram histplot
figure = plt.figure(figsize = (3,2))
sns.histplot(df_hk['age'], kde=True) 
#kde:ì¶”ì²­ì„  ë³´ì—¬ì£¼ê¸°
sns.histplot(x='age', data=df_hk, kde=True, bins=30)
```
 <br>

## #2. distplot

<b>[seaborn]</b> <br>
   ```python
   # seaborn histogram distplot í™•ë¥ ë°€ë„í•¨ìˆ˜(ì¶”ì •ì„ )
figure = plt.figure(figsize = (3,2))
sns.distplot(df_hk['age'])
```
 <br>

## #3. countplot
<b>[seaborn]</b> <br>
   ```python
plt.figure(figsize=(4, 3))
sns.countplot(x='company', data=df_hk)
```
 <br>

## #4. barplot
<b>[matplotlib]</b> <br>
```python
# bar ê°’ì„ ë§Œë“¤ì–´ì•¼ í•¨

plt.figure(figsize=(4, 3))
a_mean = df_hk[df_hk['company'] == 'A'].salary.mean()
b_mean = df_hk[df_hk['company'] == 'B'].salary.mean()
c_mean = df_hk[df_hk['company'] == 'C'].salary.mean()
X = df_hk['company'].unique()

plt.bar(x=X, height=[a_mean,b_mean,c_mean])
plt.xticks([0,1,2],['company A','company B','company C']) # plt.xticks([0,1,2] ëˆˆê¸ˆê°„ê²©
plt.show()
```
<b>[seaborn]</b> <br>
* seaborn barplot yê°’ default meanìœ¼ë¡œ ê³„ì‚° (sum, median ë³€ê²½ ê°€ëŠ¥)

```python
plt.figure(figsize=(3,2))
sns.barplot(data = df_hk, x= 'company', y= 'salary')


#confidence intervalì„ ì—†ì• ê³ , colorë¥¼ greenìœ¼ë¡œ í†µì¼, í‰ê· ì™¸ì— ì´í•©/ ì¤‘ì•™ê°’ìœ¼ë¡œ í‘œí˜„. 
# hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™” (ë‚¨/ì—¬ ë°ì´í„°ë¡œ í•œë²ˆ ë” ë‚˜ëˆ„ê¸°) #x,yì¶• ë°ì´í„° ë°”ê¿”ì£¼ë©´ ì¶• ë°”ê¿”ì„œ ê·¸ë ¤ì¤Œ
# estimator= np.median, np.sum

plt.figure(figsize=(3,2))
sns.barplot(data = df_hk, x= 'company', y= 'salary', estimator='sum', ci=None ,color='green', hue='gender')

```
## #5. boxplot
<b>[matplotlib]</b> <br>
```python
# matplotlib boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)
plt.figure(figsize=(3,4))
plt.boxplot(x= df_hk[['age','height']],data=df_hk)
plt.show

# matplotlib boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)
a_age = df_hk[df_hk['company']=='A']['age']
b_age = df_hk[df_hk['company']=='B']['age']
c_age = df_hk[df_hk['company']=='C']['age']

plt.figure(figsize=(3,4))
plt.boxplot([a_age, b_age, c_age])
plt.show
```

<b>[seaborn]</b> <br>
```python
# seaborn boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)ì— ëŒ€í•œ 4ë¶„ìœ„ê°’ì„ í‘œí˜„
plt.figure(figsize=(3,4))
sns.boxplot(data=df_hk, x= 'company' ,y='age')

# hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™”
plt.figure(figsize=(3,4))
sns.boxplot(data=df_hk, y= 'company' ,x='age', hue='gender')
```
<br>

## #6. Pie Chart
<b>[matplotlib]</b> <br>
```python
plt.figure(figsize=(3,4))
a_count = df_hk[df_hk['company'] == 'A'].company.count()
b_count = df_hk[df_hk['company'] == 'B'].company.count()
c_count = df_hk[df_hk['company'] == 'C'].company.count()

plt.pie(x = ([a_count,b_count,c_count]), 
        labels=(['company A','company B','company C']), autopct='%.1f%%') # autopct ì „ì²´ ë°±ë¶„ìœ¨, ' %.2f 'ëŠ” ì†Œìˆ«ì  2ìë¦¬

plt.show()
```
## #7. Scatter Plot
<b>[seaborn]</b> <br>

```python
#hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™”
sns.scatterplot(df_hk, x= 'age', y='salary',hue='company')
```
<b>[python-plot]</b> <br>
```python
plt.figure(figsize=(3,4))
df_hk.plot.scatter(x= 'age' , y='salary')
```

## #8. heatmap
<b>[seaborn]</b> <br>
```python
#DataFrameì˜ corr()ì€ ìˆ«ìí˜• ê°’ë§Œ ìƒê´€ë„ë¥¼ êµ¬í•¨> ìƒê´€ë„ë¥¼ ë¨¼ì € êµ¬í•´ì•¼ë¨

# annotation(ì£¼ì„) ì¸ìë¡œ ìƒê´€ê³„ìˆ˜ í‘œì‹œ
plt.figure(figsize=(4,4))
corr=  df_hk.corr()
sns.heatmap(corr, annot = True)
#DataFrameì˜ corr()ì€ ìˆ«ìí˜• ê°’ë§Œ ìƒê´€ë„ë¥¼ êµ¬í•¨

```
<br><br>

# ê°€ì„¤ê²€ì •

## <b>T-TEST ê²€ì •</b>
> ë‘ ì§‘ë‹¨ê°„ì˜ í‰ê· ë¹„êµ

<br><br>

<b>1.  t_test_1sample</b>

* t-testë¥¼ í•  dataì˜ mean ê·¼ì²˜ì˜ ê°’ìœ¼ë¡œ t-testí›„ tí†µê³„ëŸ‰ê³¼ p_value ê´€ì°° #ëª¨ì§‘ë‹¨ í‰ê· ì„ ì•Œê³  ìˆìŒ
* <b> popmean: ëª¨ì§‘ë‹¨ì˜ ì¶”ì •ëª¨ìˆ˜  ì¦‰, ğ’â‚</b> <br>
*  ëª¨í‰ê· ê³¼ ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ í†µê³„ëŸ‰ì€ ë‚®ì•„ì§€ê³  p-valueê°€ ì˜¬ë¼ê°„ë‹¤. <br>
*  Scipy -ttest_1samp() ì‚¬ìš© <br>
```í™œìš©: ê°ë‹¨ê°€ í‰ê· ì´ 00ë¼ê³  ì•Œë ¤ì ¸ìˆë‹¤. ì‚¬ì‹¤ì¸ê°€?```
  
```python 
from scipy.stats import ttest_1samp
ttest_1samp(df_hk['age'], popmean = 39.24)[1] < 0.05  

# ê²°ê³¼ê°’ : TtestResult(statistic=0.0, pvalue=1.0, df=249)
1.0 < 0.05
# ê²°ê³¼ : False, ê²°ê³¼ í•´ì„: 95% ì‹ ë¢°ìˆ˜ì¤€ìœ¼ë¡œ 100% ì¼ì¹˜
```

<br><br>
<b>2. Two sample t-test</b>

* ë‹¤ë¥¸í™˜ê²½ (ì£¼ë§/ì£¼ì¤‘) ì¦‰, ë…ë¦½ëœ ëª¨ì§‘ë‹¨ì—ì„œ ì¶”ì¶œëœ ê° ë‘ ì§‘ë‹¨ê°„ í‰ê· ì´ ê°™ì€ì§€ ê²€ì •
* ë“±ë¶„ì‚° ì—¬ë¶€ì—ë”°ë¼ ê²€ì •í†µê³„ëŸ‰ ê³„ì‚°ì‹ì´ ë‹¬ë¼ì„œ ë“±ë¶„ì‚° ê²€ì •ì„ í•´ì¤˜ì•¼í•œë‹¤.
* ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° wilcoxon rank sum test (ìˆœìœ„í•©ê²€ì •) ì‚¬ìš©
* ë“±ë¶„ì‚° ê°€ì •ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°, equal_var ì¸ìì— Trueë¥¼ í• ë‹¹
* ttest_ind() ì‚¬ìš© <br>
```í™œìš©: ì£¼ì¤‘ ê°ë‹¨ê°€ì™€ ì£¼ë§ ê°ë‹¨ê°€ê°€ ê°™ì€ê°€?```

```python
from scipy.stats import ttest_ind
ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary)

#ê²°ê³¼ : Ttest_indResult(statistic=5.941362455469809, pvalue=1.2532322871358408e-08)

```


2-1.  sample t-test (A>=B) #less_ A(ğ’â‚€)ë³´ë‹¤ B(ğ’â‚)ê°€ ì‘ë‹¤ (í•˜ë‹¨ì¸¡ê²€ì •)
   ```python
   ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary,
         alternative='less')

   #ê²°ê³¼ê°’: Ttest_indResult(statistic=5.941362455469809, pvalue=0.9999999937338386)
   ```

2-2. sample t-test (A<=B) #greater_ A(ğ’â‚€)ë³´ë‹¤ B(ğ’â‚) ê°€ í¬ë‹¤(ìƒë‹¨ì¸¡ê²€ì •)
   ```python
   ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary,
            alternative='greater')

   #Ttest_indResult(statistic=5.941362455469809, pvalue=6.266161435679204e-09)
 ```

 > ì˜ˆì œ 
 1) iris ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬('iris.csv') species column 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14ì™€ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ì‹œì˜¤
      ```python
      # H0 : 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14 ì™€ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤ (ìœ ì˜ìˆ˜ì¤€ 0.05)
      # H1 : 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14 ì™€ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ì—†ë‹¤ (ìœ ì˜ìˆ˜ì¤€ 0.05)
      #df_iris['sepal_width'].mean()
      from scipy.stats import ttest_1samp, ttest_ind

      ttest_1samp(df_iris['sepal_width'], popmean = 3.14)[1] < 0.05
      #ê²°ê³¼ê°’:  True = H1 ì§€ì§€, H0 ê¸°ê°
      ```
 2) 'setosa'ì™€ 'versicolor'ì˜ sepal_length í‰ê· ì´ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ì‹œì˜¤
      ```python
      from scipy.stats import ttest_1samp, ttest_ind
      a = df_iris[(df_iris['species']=='setosa')]['sepal_length']
      b = df_iris[(df_iris['species']=='versicolor')]['sepal_length']

      ttest_ind(a,b)[1] <0.05
      #ê²°ê³¼ê°’ : True = H1 ì§€ì§€, H0 ê¸°ê°
      ```
<br>
<b>3. Paired sample t-test</b>

 * ë™ì‹œê°„ëŒ€ ë“± í™˜ê²½ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì¤€ ë™ì¼í•œ ëª¨ì§‘ë‹¨ì—ì„œë¶€í„° ì¶”ì¶œëœ  ë‘ ì§‘ë‹¨ê°„ ì°¨ì´ê°€ ìˆëŠ”ì§€ ê²€ì •
 * í‘œë³¸ì´ ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° wilcoxon rank sum test (ìˆœìœ„í•©ê²€ì •) ì‚¬ìš©
 * Scipy -ttest_rel() ì‚¬ìš©
 * ```í™œìš©: ë™ë…„, ë™ì›”, ë™ì‹œê°„ëŒ€ ì´ìš©ê° ëŒ€ìƒìœ¼ë¡œ ë¹„íšŒì›ì˜ ëŒ€ì—¬ëŸ‰ê³¼ íšŒì›ì˜ ìì „ê±° ëŒ€ì—¬ëŸ‰ì˜ í‰ê· ì°¨ì´ ê²€ì •```



<br><br><br>

## <b>ANOVA ê²€ì •</b> 

* ìˆ˜ì¹˜ê°€ í†µê³„ì ìœ¼ë¡œ ë™ì§ˆì ì¸ì§€ ì´ì§ˆì ì¸ì§€ ê²€ì¦í•˜ê¸° ìœ„í•´ í˜„ì—…ì—ì„œ ë§ì´ ì‚¬ìš©ë¨ <br>
* ê·¸ë£¹ì´ 2ê°œ ì´ìƒ ì¼ë•Œ ì‚¬ìš©í•˜ë©°, 2ê°œì¸ ê²½ìš°ëŠ” t-testì˜ ê²°ê³¼ê°’ê³¼ ë™ì¼í•˜ê²Œ ë‚˜ì˜¨ë‹¤. <br>

<b> 1. ì¼ì›ë¶„ì‚°ë¶„ì„ (One way Anova)</b>
* ì„¸ì§‘ë‹¨ ì´ìƒ ê°„ì˜ í‰ê· ì°¨ì´
* ìˆ˜ì¹˜í˜• ì¢…ì†ë³€ìˆ˜(í‰ê· )ì™€ ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ ê°ê°1í•˜ë‚˜ì”©ì¼ë•Œ ì‹¤ì‹œ
* (ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ ì˜ˆì‹œ: company ê°™ì€ ì§‘ë‹¨)
* ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ ë¬¸ìí˜•ì¸ê²½ìš°ì—ëŠ” ìƒê´€ì—†ì§€ë§Œ, 
  1,2,3,4 ì²˜ëŸ¼ êµ¬ë¶„ë˜ì–´ìˆëŠ” ê²½ìš° ìˆ«ìí˜•ìœ¼ë¡œ ì¸ì‹í•˜ê²Œí•˜ì§€ ì•Šê¸° ìœ„í•´, ì¼ë°˜ì ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ ì•ì— Cë¡œ ê°ì‹¸ì¤€ë‹¤.
   ```python
   model= ols(formula = 'price ~ C(color)', data = df).fit()
   anova_lm(model)
   ```
  ```í™œìš©: ê¸ˆ,í† ,ì¼ì´ ê°™ì€ 'ì£¼ë§'ì¸ì§€ or 80ì ê³¼ 81ì ì´ ë™ì¼í•œ ìˆ˜ì¤€ì¸ì§€ ë“±```

<br>

<b>[scipy]</b> <br>
```python
# ANOVA scipy.stats ì‚¬ìš©
from scipy.stats import f_oneway

##sample test ì¤€ë¹„í•„ìš”
a= df_hk[df_hk['company'] =='A'].salary
b= df_hk[df_hk['company'] =='B'].salary
c= df_hk[df_hk['company'] =='C'].salary

f_oneway(a,b,c)[1] < 0.05 #H0ê¸°ê° :íšŒì‚¬ì™€ ì—°ë´‰ì€ ì—°ê´€ì´ ìˆë‹¤.
```
<b>[statsmodels]</b> #ë¶„ì‚°ë¶„ì„í‘œ ì œê³µ
```python
# ANOVA statsmodels ì‚¬ìš©
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

model = ols(formula = 'salary~ company', data= df_hk).fit() #formula: ì¢…ì†ë³€ìˆ˜~ë…ë¦½ë³€ìˆ˜ 

anova_lm(model)
```
<br><br>
<b>2. ì´ì›ë¶„ì‚°ë¶„ì„(Two way Anova)</b>
* ìˆ˜ì¹˜í˜• ì¢…ì†ë³€ìˆ˜ 1ê°œ, ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œì¼ë•Œ ì‹¤ì‹œ
* ì£¼ìš”íš¨ê³¼ + êµíš¨íš¨ê³¼ (ë…ë¦½ë³€ìˆ˜ê°™ì˜ ì˜í–¥ì„±) í•¨ê»˜ ë¶„ì„
* ì£¼ìš”íš¨ê³¼ ê·€ë¬´ê°€ì„¤ : í‰ê· ì´ ê°™ìŒ
* êµí˜¸ì‘ìš©íš¨ê³¼ ê·€ë¬´ê°€ì„¤:  ìš”ì¸ê°„ êµí˜¸ì‘ìš©ì´ ì—†ìŒ 
* : ë¥¼ ì´ìš© (ì£¼ìš”íš¨ê³¼ ë’¤ +ë¡œ ì—°ê²°)

```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
model = ols(formula = 'price ~ cut + color + cut:color', data=df).fit()
anova_lm(model)

#ê²°ê³¼í•´ì„: ë‹¤ì´ì•„ëª¬ë“œì˜ cutë³„ ë˜ëŠ” colorë³„ê°„ì— price í‰ê· ì´ ê°™ìœ¼ë©°(ì°¨ì´ ì—†ìŒ), cut-colorê°„ êµí˜¸ì‘ìš©ë„ ì—†ë‹¤.
```
```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

model = ols('registered ~ C(season)+C(holiday) + C(season):C(holiday)',
           data= df_bike).fit()
anova_lm(model).round(5)
```








## <b>ì‚¬í›„ê²€ì •_tukey</b> 
* ë…ë¦½2í‘œë³¸ t-ê²€ì •ê³¼ ìœ ì‚¬
* reject : ìœ ì˜ìˆ˜ì¤€ ì•ˆì—ì„œ 'ê·€ë¬´ê°€ì„¤' ê¸°ê° ì—¬ë¶€ 
  (False : ê¸°ê° ë¶ˆê°€ /  True : reject (ê¸°ê°))
* True : ê·€ë¬´ê°€ì„¤ ê¸°ê°ì´ë‹ˆê¹Œ ë‘ ì§‘ë‹¨ì€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒ.
  
```python
#endog : y label
#alpha : ìœ ì˜ ìˆ˜ì¤€ 0.05
from statsmodels.stats.multicomp import pairwise_tukeyhsd

posthoc = pairwise_tukeyhsd(df_hk['salary'], df_hk['company'], alpha =0.05 ) #ì¢…ì†,ë…ë¦½ ìˆœ

print(posthoc) #ë³€ìˆ˜ì— í• ë‹¹í•´ì„œ í”„ë¦°íŠ¸ í•„ìš”
```


<br>

> ì˜ˆì œ

bike ë°ì´í„°(bike.cvs)ë¥¼ ì‚¬ìš©í•˜ì—¬, ìš”ì¼ë³„ registered í‰ê· ì´ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ë‹ˆ,

í‰ê· ì´ ê°™ì§€ ì•Šì„ë•Œ, í‰ê· ì´ ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ì°¨ì´ë‚˜ì§€ ì•ŠëŠ” ì¡°í•©(False)ì€ ëª‡ ê°œì¸ê°€ ?
```python
from statsmodels.stats.multicomp import pairwise_tukeyhsd

post_hoc = pairwise_tukeyhsd(df_bike['registered'], df_bike['date'], alpha = 0.05)
print(post_hoc)
```

<br>

## <b>ìƒê´€ë¶„ì„</b>
- ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ ì„ í˜•ê´€ê³„ ì—¬ë¶€ë¥¼ ë¶„ì„
- Pandas-corr()ë©”ì„œë“œ : ìƒê´€ê³„ìˆ˜ë§Œ ì œê³µ
- Scipy í•¨ìˆ˜ : ìƒê´€ê³„ìˆ˜ì™€ p-value í™•ì¸ ê°€ëŠ¥ <br>
    (ê²€ì •í†µê³„ëŸ‰ì„ ë‚´ë¶€ì ìœ¼ë¡œ ê°€ì§€ê³  ìˆì§€ë§Œ ë³´ì—¬ì§€ëŠ” ìˆ«ìëŠ” ê²€ì •í†µê³„ëŸ‰ì´ ì•„ë‹Œ ìƒê´€ê³„ìˆ˜ì™€ PV)
- ê²€ì • í†µê³„ëŸ‰ : t
- ê±°ì˜ pearsonì´ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, spearnam,kendallì˜ ìˆœì„œí˜• ìƒê´€ë¶„ì„ì€ ëŒ€í‘œì ìœ¼ë¡œ ì„¤ë¬¸ì¡°ì‚¬ (ë‚˜ì¨/ ì¢‹ìŒ /ë§¤ìš°ì¢‹ìŒ ë“±)ì— ì‚¬ìš©ë¨
  



<b>[corr()]</b> <br>
```python
# pearsonr, spearmanr, kendalltau
df_hk.corr() #default method = pearson
df_hk.corr(method='pearson')
df_hk.corr(method='kendall')
df_hk.corr(method='spearman')

#groupbyë¥¼ í™œìš©í•œ ìƒê´€ë¶„ì„
df2= df.groupby('weather')[['temp','casual']].corr()
df2 = df2.reset_index()
df2[(df2['atemp'])<1]

```
<b>[scipy]</b> <br>
```python
# pearsonr
from scipy.stats import pearsonr
pearsonr(df_hk['age'], df_hk['salary'])

# spearmanr
from scipy.stats import spearmanr
spearmanr(df_hk['age'], df_hk['salary'])

# kendalltau
from scipy.stats import kendalltau
kendalltau(df_hk['age'], df_hk['salary'])
```


> ì˜ˆì œ 
 <br>
1.
   temp, atemp, humidity, registeredì˜ ìƒê´€ ê³„ìˆ˜ì¤‘ ê°€ì¥ ë†’ì€ê²ƒì€ ?
   ```python
   df_bike[['temp','atemp','humidity','registered']].corr()
   ``` 
<br>

1. ë‚ ì”¨ê°€ ë§‘ì€ë‚ (weather = 1) ê³¼ ê·¸ë ‡ì§€ ì•Šì€ë‚  ì˜¨ë„(temp)ì™€ ìì „ê±° ëŒ€ì—¬ ìˆ«ì(casual)ì˜ ìƒê´€ê³„ìˆ˜ì˜ ì ˆëŒ€ê°’ì€ ì–¼ë§ˆì¸ê°€ ?
      ```python
      df_bike[(df_bike['weather'] == 1)][['temp','casual']].corr()
      ```
```!! corrëŠ” DFë¡œ ì¶”ì¶œí•´ì•¼í•¨ [[]] !!```


<br>

## <b>ë…ë¦½ì„±(ì—°ê´€ì„±) ê²€ì •</b> ```[ì¹´ì´ì œê³±ê²€ì •]```

   - ì í•©ë„, ë…ë¦½ì„±, ë™ì§ˆì„± ê²€ì • ì‚¬ì „ì— ì§„í–‰ í•„ìš”
   - ê²€ì •í†µê³„ëŸ‰ : ì¹´ì´ì œê³± âˆ‘ ((ê´€ì¸¡ë„ìˆ˜ - ê¸°ëŒ€ë„ìˆ˜)Â² / ê¸°ëŒ€ë„ìˆ˜)
   - crosstab (ë¹ˆë„í‘œ) ìë£Œ í™œìš© í•„ìš”
   - ë‘ ëª…ëª©í˜• ìë£Œ ê²€ì • <br>
     (ìˆ˜ì¹˜í˜•ì¼ ê²½ìš°ëŠ” êµ¬ê°„í™”/ë²”ì£¼í™”ë¥¼ í†µí•´ì„œ ëª…ëª©í˜•ìœ¼ë¡œ ë³€í™˜í›„ ì‚¬ìš©)
   - correction = False ëŠ” ì—°ì†ì„± ìˆ˜ì •ì„ ì ìš©í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•¨ ```chi2_contingency(crosstab2 , correction = False)```
 <br>

```python
# ê´€ì¸¡ê°’ cross ì œê³µ í•„ìš”

cross = pd.crosstab(df_hk['gender'], df_hk['company'])
#margins = True : í–‰/ì—´í•©ê³„, normalize = True: ì „ì²´ê¸°ì¤€

from scipy.stats import chi2_contingency #ë…ë¦½ì„±ê²€ì •
chi2_contingency(cross)

#ê²°ê³¼ê°’ dof =ììœ ë„
Chi2ContingencyResult(statistic=1.674107142857143, pvalue=0.43298440342651534, dof=2, expected_freq=array([[44.8, 44.8, 22.4],
       [55.2, 55.2, 27.6]

pvalue=0.43298440342651534 > 0.05
#H0 ì±„íƒ: ë…ë¦½ì´ë‹¤(ìƒê´€ì—†ë‹¤)
```

> ì˜ˆì œ

1) seasonê³¼ weather dtypeì„ ë¬¸ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
ë‘ ë³€ìˆ˜ê°€ ê´€ë ¨ìˆëŠ”ì§€ ì ì ˆí•œ ê²€ì •ì„ í•˜ê³  ê²€ì •í†µê³„ëŸ‰ê³¼ p-valueë¥¼ êµ¬í•˜ì‹œì˜¤
   ```python
   df_bike['weather']= df_bike['weather'].astype('str')
   df_bike['season']= df_bike['season'].astype('str')
   cross = pd.crosstab(df_bike['weather'], df_bike['season'])
   chi2_contingency(cross)[1] < 0.05
   #h0 ê¸°ê°
   ```

2) ìì „ê±° ì´ ëŒ€ì—¬ìˆ˜(count)ê°€ ìƒìœ„ 30%ì¼ë•Œ 'high', ê·¸ ë¯¸ë§Œ ì¼ë•Œ 'low' ì¸ íŒŒìƒë³€ìˆ˜(count_high)ë¥¼ ìƒì„±í•˜ê³ 
count_highì™€ workingdayì˜ ì—°ê´€ì„± ì—¬ë¶€ë¥¼ ê²€ì •í•˜ê³  ê²€ì • í†µê³„ëŸ‰ì„ êµ¬í•˜ì‹œì˜¤ (ì†Œìˆ«ì  ë„·ì§¸ìë¦¬ ë°˜ì˜¬ë¦¼í•˜ì—¬ í‘œê¸°)
   ```python
   import numpy as np
   df_bike['workingday']= df_bike['workingday'].astype('str')

   df_bike['count_high'] = np.where(df_bike['count']>= df_bike['count'].quantile(0.3), 'high','low')
   cross = pd.crosstab(df_bike['count_high'], df_bike['workingday'])
   chi2_contingency(cross)[0].round(4)
   ```


<br><br>

## ë“±ë¶„ì‚° ê²€ì •
* ë‘ ì§‘ë‹¨ ë˜ëŠ” ê·¸ì´ìƒì˜ ì§‘ë‹¨ê°„ ë¶„ì‚°ì´ ê°™ì€ì§€ ì—¬ë¶€ ê²€ì •
* ê·€ë¬´ê°€ì„¤ : ì§‘ë‹¨ê°„ ë¶„ì‚°ì€ ì„œë¡œ ê°™ìŒ

  1. f_test : ë‘ì§‘ë‹¨ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì‚¬ìš© 
     (Anova ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„)
      
      <b>pythonì—ëŠ” fê²€ì • í•¨ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì—
      Scipy - f.cdf() (ëˆ„ì ë¶„í¬ê´€ë ¨ í•¨ìˆ˜)ì—
      f ê²€ì •í†µê³„ëŸ‰ (ë¶„ì‚°ë¹„), ì²«ë²ˆì§¸ ë°ì´í„°ì˜ ììœ ë„, ë‘ë²ˆì§¸ ë°ì´í„°ì˜ ììœ ë„ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì—¬ p-valueë¥¼ êµ¬í•œë‹¤.</b>
  2. Bartlett's test : ë‘ì§‘ë‹¨ ì´ìƒ ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì‚¬ìš©
  3. Lavene's test : ë‘ì§‘ë‹¨ ì´ìƒ ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ í•„ìš”ê°€ ì—†ìŒ

> ì˜ˆì œ

1) ë‚¨ì„±ê³¼ ì—¬ì„±ì˜ 1íšŒ í‰ê·  ì†¡ê¸ˆì•¡ì˜ ë¶„ì‚°ì„ ë¹„êµê²€ì •í•˜ê³ , ê·¸ ê²°ê³¼ì˜ ê²€ì •í†µê³„ëŸ‰ì€ ì–¼ë§ˆì¸ê°€?
(2ì§‘ë‹¨ì´ë‹ˆê¹Œ ë¶„ì‚°ë¹„ë¥¼ êµ¬í•´ì„œ Fê²€ì •ë„ ê°€ëŠ¥)



   ```python
   from scipy.stats import f
   from scipy.stats import bartlett
   from scipy.stats import levene

   #ê³µí†µì „ì²˜ë¦¬
   df['trans_mean'] = df['Total_trans_amt'] / df['Total_trans_cnt']
   M_a =df[(df['Gender']== 'M')]['trans_mean']
   F_a =df[(df['Gender']== 'F')]['trans_mean']

   #---Fê²€ì •---

   #Fí†µê³„ëŸ‰
   F = M_a.var() / F_a.var()
   print(F)

   #fê²€ì •_cdf
   result = f.cdf(F, dfd = len(M_a)-1, dfn = len(F_a)) 
   print(result)

   #p-value
   p = (1-result) * 2


   #---bartlett ê²€ì •---
   bartlett(M_a, F_a)


   #---levene ê²€ì •---
   levene(M_a, F_a)
   ```

<br><br>

## ì‹œê³„ì—´ë¶„ì„
### <b>í‰í™œí™”</b>
 - ì´ë™í‰ê· ë²• 
   -  ë‹¨ìˆœì´ë™í‰ê· ë²• (Simple Moving Average)
  
      ë©”ì„œë“œ : Pandas =rolling() <br>
      windowì—ëŠ” ì´ë™í‰ê· ëŒ€ìƒì´ ë˜ëŠ” ë°ì´í„° ê°œìˆ˜ n ë¥¼ ì§€ì • <br>
      nì¼ë¶€í„° í‰ê· ì¹˜ ê³„ì‚°ë¨ìœ¼ë¡œ ìµœì´ˆë°ì´í„° n-1ê°œ ë§Œí¼ ê²°ì¸¡ì¹˜ ì¡´ì¬ <br>
      centerì— Trueë¥¼ ì…ë ¥í•  ê²½ìš° ì¤‘ì‹¬ ì´ë™ í‰ê· ì‹¤ì‹œ ê°€ëŠ¥ <br>

      ```python
      import pandas as pd
      df = pd.read_csv('./data/seoul_subway.csv')

      df_sub["mean5"] =df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'].rolling(window=5).mean()
      df_sub[:10]
      ```


   -  ê°€ì¤‘ì´ë™í‰ê· ë²• (Weighted Moving Average)
  <br><br>

 - ì§€ìˆ˜í‰í™œë²• 
   -  ë‹¨ìˆœì§€ìˆ˜í‰í™œë²•(EWMA)  
      ë©”ì„œë“œ : Pandas-ewm() <br> 
      Alpha = ì§€ìˆ˜í‰í™œê³„ìˆ˜ ì…ë ¥
      ```python
      df_sub["EWMA"] =df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'].ewm(alpha = 0.9).mean()
      df_sub[:10]
      ```


   -  ì´ì¤‘ì§€ìˆ˜í‰í™œë²•(Winters) 
   -  ì‚¼ì¤‘ì§€ìˆ˜í‰í™œë²•(HoltWinters)


### <b>ì‹œê³„ì—´ë¶„í•´</b>


- ë©”ì„œë“œ : statsmodels - seasonal_decompose()
- Model ì¸ìì— 'multiplicative'ë¥¼ ì…ë ¥í•˜ë©´ ìŠ¹ë²•ëª¨í˜• ì ìš©(ê¸°ë³¸ = ê°€ë²•ëª¨í˜•
- :star: ì‹œê³„ì—´ ë°ì´í„° ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤í™” í•´ì¤˜ì•¼í•œë‹¤.

```python
from statsmodels.tsa.seasonal import seasonal_decompose
#tsa : time series analysis

df['ì‚¬ìš©ì¼ì'] = pd.to_datetime(df['ì‚¬ìš©ì¼ì'], format = '%Y%m%d')
df_sub = df[(df['ë…¸ì„ ëª…'] =='1í˜¸ì„ ')&(df['ì—­ëª…'] =='ì¢…ë¡œ3ê°€')]
df_sub = df_sub.set_index('ì‚¬ìš©ì¼ì') #ì‹œê³„ì—´ ë°ì´í„° ì¸ë±ìŠ¤í™” : ë°ì´í„°ë ˆë²¨ë¡œ ì ‘ê·¼

result = seasonal_decompose(df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'][:200])
result.plot()
#result.seasonal
#result.trend 
#result.resid 
```


<br><br>

# Machine Learning

## <b>Pre-processing</b>
### <b>1. Scaling</b>

 - ìŠ¤ì¼€ì¼ë§ ì „í›„ ë¹„êµë¥¼ ìœ„í•´ histogram 2ê°€ì§€

   ```python
   df_hk.hist()

   sns.histplot(df_hk[['height','age', 'salary' , 'expenditure']])
   ```



 -  min_max scaling (ìµœì†Œ-ìµœëŒ€ ë³€í™˜) (ë²”ìœ„:0~1) <br>
      ```python
      from sklearn.preprocessing import MinMaxScaler, StandardScaler
      m_scaler = MinMaxScaler()

      model = m_scaler.fit(df_hk[['height','age', 'salary' , 'expenditure']])
      x_train_scaled = model.transform(df_hk[['height','age', 'salary' , 'expenditure']])
      df_hk_minmax = pd.DataFrame(x_train_scaled, columns= ['height','age', 'salary' , 'expenditure'])
      df_hk_minmax 
      ```

 -  standard scaling (Z-score ë³€í™˜) (ë²”ìœ„ : 0 ì¤‘ì‹¬, -âˆ ~ +âˆ)
      ```python
      from sklearn.preprocessing import MinMaxScaler, StandardScaler
      s_scaler = StandardScaler()

      model = s_scaler.fit(df_hk[['height','age', 'salary' , 'expenditure']])
      x_train_scaled = model.transform(df_hk[['height','age', 'salary' , 'expenditure']])
      df_hk_stand = pd.DataFrame(x_train_scaled, columns= ['height','age', 'salary' , 'expenditure'])
      df_hk_stand 
      ```
<br>

### <b>2. Get dummies</b>

```python
# í•´ë‹¹ columnë§Œ get_dummies
pd.get_dummies(df[['gender', 'blood_type', 'company', 'grades']],drop_first= True)  

#ì „ì²´ ë°ì´í„°ì— get_dummies
pd.get_dummies(df, columns =['gender', 'blood_type', 'company', 'grades'], drop_first= True)  
```
<br><br>

## <b>ê³„ì¸µì  êµ°ì§‘ë¶„ì„</b>
- ë°ì´í„°ë³€ë™ì— ë¯¼ê°
- ê±°ë¦¬ê¸°ë°˜(ìœ ì‚¬ë„)ë¡œ ë¬¶ê¸°
- ë°ì´í„°ê°œìˆ˜ê°€ ë§ì€ê²½ìš° ì—°ì‚°ì— ë§ì€ ì‹œê°„ì´ ì†Œìš” (5ì²œê°œ~ë§Œê°œë¥¼ ë„˜ê¸°ëŠ”ë°ì´í„°ì—ëŠ” ë¹„ê¶Œì¥)
- ê³„ì¸µë„(Dendrogram): ë°ì´í„°ê°„ ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì‹í™”í•œ ë„í‘œ
- [ë©”ì„œë“œ] 
  - sklearn-AgglomerativeClustering() <br>
      - n_clusters = ë¶„ë˜ êµ°ì§‘ê°œìˆ˜ ì„¤ì • <br>
      - Affinity = ë°ì´í„° ê°„ ê±°ë¦¬ê³„ì‚°ë°©ë²• <br>
      - Linkage = êµ°ì§‘ê°„ ìœ ì‚¬ë„ ë°©ë²• ì„¤ì • <br>


  - Spicy - dendrogram() 
  - Spicy - Linkage() =  ë°ì´í„° ê°„ ê±°ë¦¬ ê³„ì‚° ë° êµ°ì§‘ í˜•ì„±
- ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³µì‹ : ë‘ ê±°ë¦¬ diffë¥¼ ì œê³±í•˜ê³  ë”í•˜ì—¬ ë§ˆì§€ë§‰ì— ë£¨íŠ¸ë¥¼ ì”Œì›Œì¤Œ
```python
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt

df = pd.read_csv('./data/iris.csv')
df_x = df[['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width']]
model = AgglomerativeClustering(n_clusters = 3).fit(df_x) 
# y labelì´ ì—†ëŠ” ë¹„ì§€ë„í•™ìŠµ > ë…ë¦½ë³€ìˆ˜ë§Œ fit ì‹œí‚´
# (default) method = ward 
df['cluster'] = model.labels_
pd.crosstab(df['Species'], df['cluster'])

df.groupby('cluster').mean().reset_index()
# êµ°ì§‘ë³„ centroidêµ¬í•˜ê¸°


# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
link = linkage(df_x, method = 'ward') 
#(default) method = single ìœ„ ëª¨ë¸ê³¼ ì¼ì¹˜ ì‹œì¼œì¤Œ > wardë¡œ ë³€ê²½
dendrogram(link)
plt.show()
```

## <b>ë¹„ê³„ì¸µì  êµ°ì§‘ë¶„ì„</b>
- ì„ì˜ì˜ kì ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ê¹Œìš´ ê±°ë¦¬ì˜ ë°ì´í„°ë¥¼ ë¬¶ìŒ
- kë¥¼ í™•ì •í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ í•„ìš”
- ê²°ê³¼ë¥¼ ê³ ì •í•˜ê¸° ìœ„í•´ seedì„¤ì • í•„ìš”
```python
import pandas as pd
df = pd.read_csv('./data/iris.csv')
from sklearn.cluster import KMeans

model = KMeans(n_clusters = 3, random_state = 123).fit(df.iloc[:,:-1])

df['cluster'] = model.labels_
df.groupby('cluster').mean() # model.cluster_centers_ ì™€ ê¸°ëŠ¥ë™ì¼, ë‹¨ í•´ë‹¹ ì½”ë“œëŠ” ì»¬ëŸ¼ì´ ì—†ì–´ ë³„ë„ dfì‘ì—…ì„ í•´ì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì—, groupbyë¥¼ ì§ì ‘ í•´ì£¼ëŠ” ê²ƒì´ ë” í¸ë¦¬
```
<br><br>

## <b>ë‹¨ìˆœì„ í˜•íšŒê·€</b>
 - statsmodels vs sklearn ë¹„êµ 
 - statsmodelsëŠ” í†µê³„ê¸°ë°˜ ê´€ì   (summary í‘œ ë“± í†µê³„ìë£Œ ë³´ê¸° í¸í•¨) 
 - sklearnëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê´€ì 
 - ì…ë ¥ê°’ì˜ ì°¨ì´(statemodels olsì˜ ê²½ìš° formula ë¬¸ë²•ì´ ìˆìŒ / sklearnëŠ” fit() í™œìš©)
-  ols model ê³¼ sklearn ëª¨ë¸ ë‘ê°€ì§€ë¡œ ì ‘ê·¼ ê°€ëŠ¥
-  íšŒê·€ olsëŠ”  ì—°ì†í˜• ì¢…ì†ë³€ìˆ˜, ì—°ì†í˜• ë…ë¦½ë³€ìˆ˜
   ANOVA ols ëŠ” ì—°ì†í˜• ì¢…ì†ë³€ìˆ˜, ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜
-  ols.summary()ì—ì„œ fpvalueë¡œ ì„ í˜•ì„± ê²€ì¦
-  olsëŠ” í•™ìŠµí•œ ë°ì´í„°ë§Œ ë³„ë„ë¡œ ëª¨ë¸ì— ì í•©ì‹œí‚¤ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ ê±¸ëŸ¬ì„œ ì í•©í•¨
- ë°˜ë©´ sklearnì€ interceptë¡œ ì ˆí¸ ì í•© ì—¬ë¶€ ì„¤ì •ì´ ê°€ëŠ¥í•˜ì—¬ ê°•ë ¥í•œ ìµœì í™” ê°•ì ì„ ê°€ì§€ê³  ìˆë‹¤

<br>

### <b> ì„ í˜•íšŒê·€ ê°€ì • 4ê°€ì§€ ì„ í˜•ì„±, ì •ê·œì„±, ë“±ë¶„ì‚°, ë…ë¦½ì„±</b>
### <b>1. ì„ í˜•ì„±</b>
- F ê²€ì •ì˜ pvalueë¡œ í™•ì¸
   ```python
   # ì„ í˜•íšŒê·€ ê·¸ë˜í”„, regplot: scatter plot, regression line, confidence bandë¥¼ í•œ ë²ˆì— ê·¸ë¦¬ëŠ” ê¸°ëŠ¥
   sns.regplot(x='salary', y='expenditure', data=df)
   sns.regplot(x=df['salary'], y=predict_ols)

   # F ê²€ì •ì˜ pvalueë¡œ í™•ì¸
   model_ols.f_pvalue 
   model_ols.f_pvalue < 0.05
   # ê²°ê³¼ : True (h1 ì§€ì§€ : ì„ í˜•ì„±ì´ ìˆë‹¤ê³  íŒë‹¨)
   ```
### <b>2. ì”ì°¨ì˜ ì •ê·œì„±</b>
- ì”ì°¨ ê·¸ë˜í”„ë¡œ í™•ì¸
- shapiro ì˜ ê²½ìš° pê°’ì´ 0.05 ì´ìƒì´ë©´ ì •ê·œì„± ë§Œì¡±í•œë‹¤ 

   ```python
   # ì”ì°¨ ê³„ì‚°
   residual = df['expenditure']-predict_ols

   # ì”ì°¨ ê·¸ë˜í”„ 1
   plt.scatter(df['salary'], residual)
   plt.show()
   # ì”ì°¨ ê·¸ë˜í”„ 2
   sns.distplot(residual)
   plt.show()

   # shapiro ì •ê·œì„± ê²€ì •, Ho: ì •ê·œì„±ì„ ê°€ì§„ë‹¤ (p-value > 0.05)
   from scipy.stats import shapiro
   shapiro(residual)[1] < 0.05
   # ê²°ê³¼í•´ì„: TRUE , 0.05 ë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°(= ì •ê·œì„±ì„ ë§Œì¡± ëª»í•¨)
   # residualì´ ê·¸ë£¹í™” ë˜ì–´ ìˆì–´ shapiro testì—ì„œ ì •ê·œì„±ì´ ì•ˆ ë‚˜ì˜´

   # ì‹œê°í™”
   import scipy.stats as stats
   stats.probplot(residual, plot=plt)
   plt.show()
   ```

### <b>3. ì”ì°¨ì˜ ë“±ë¶„ì‚°</b>
- ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨ì˜ ì‚°ì ë„ë¡œ íŒŒì•…
   ```python
   # ì”ì°¨ê·¸ë˜í”„ë¡œ í™•ì¸, Xê°€ ì»¤ì§ˆë•Œ ì”ì°¨ì˜ ê°„ê²©ì´ ë³€í•˜ë©´ ì•ˆë¨, ê°„ê²©ì´ ì¼ì •í•˜ë©´ ë“±ë¶„ì‚°ì„± ë§Œì¡±
   sns.regplot(x=predict_ols, y=residual)
   ```
### <b>4. ì”ì°¨ì˜ ë…ë¦½ì„±</b>
- ì”ì°¨ê°€ ë…ë¦½ì¸ì§€(ìê¸°ìƒê´€ì„±ì´ ìˆëŠ”ì§€) ê²€ì •
   ```python
   #perform Durbin-Watson test

   from statsmodels.stats.stattools import durbin_watson
   durbin_watson(model_ols.resid)

   # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰ì€ 0 ~ 4ì‚¬ì´ì˜ ê°’ì„ ê°–ì„ ìˆ˜ ìˆìŒ
   # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ì–‘ì˜ ìƒê´€ê´€ê³„
   # 4ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ìŒì˜ ìƒê´€ê´€ê³„
   # 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ì´ ì—†ìŒ
   ```
<br><br>

### <b> ë‹¨ìˆœì„ í˜•íšŒê·€</b>
### <b>statsmodels_ols</b>
```python
from statsmodels.formula.api import ols
model_ols = ols(formula = 'expenditure ~ salary', data= df).fit()

model_ols.summary()

model_ols.params #ì ˆí¸, íšŒê·€ê³„ìˆ˜
model_ols.resid #ì”ì°¨

# íšŒê·€ì‹ ë§Œë“¤ì–´ë³´ê¸°
def linear_ols(x):
    return (model_ols.params[1]* x + model_ols.params[0])
linear_ols(4720)

# predictë¡œ ì˜ˆì¸¡ê°’ í™•ì¸
predict_ols = model_ols.predict(df['salary'])

# ì‹œê°í™” 
fig, ax = plt.subplots( nrows= 1 , ncols=2, figsize=(14, 5))
sns.scatterplot(x=df['salary'], y=df['expenditure'], palette='Set1', ax= ax[0] )
sns.scatterplot(x=df['salary'], y=predict_ols, palette='Set2', ax=ax[1] )

ax[0].set_title('expenditure ')
ax[1].set_title('predict_ols')
plt.show()
```
### <b>statsmodels_OLS</b>
```python
# statsmodels.api
import statsmodels.api as sm

model_sm = sm.OLS(df_train1['salary'], sm.add_constant(df_train1[['age']])).fit()
model_sm

#sm.add_constant ìˆì–´ì•¼ ìƒìˆ˜í•­ì´ í¬í•¨ë¨
model_sm.predict(sm.add_constant(df_test1[['age']]))[:5]

# summary() #ìƒìˆ˜í•­í™•ì¸ê°€ëŠ¥
model_sm.summary()
```

### <b>sklearn</b></b>
```python
# LinearRegression í˜¸ì¶œ
from sklearn.linear_model import LinearRegression

# ëª¨ë¸ì„ íƒ, ë…ë¦½ë³€ìˆ˜(salary), ì¢…ì†ë³€ìˆ˜(expenditure) ì…ë ¥, fit 
model_lm = LinearRegression()
model_lm.fit(X = df[['salary']], y= df[['expenditure']])
# íšŒê·€ê³„ìˆ˜ í™•ì¸
model_lm.coef_
# intercept_ í™•ì¸
model_lm.intercept_

# íšŒê·€ì‹ ë§Œë“¤ì–´ë³´ê¸°
def linear_lm(x):
    return (model_lm.coef_[0] * x) + model_lm.intercept_
# íšŒê·€ì‹ìœ¼ë¡œ ì˜ˆì¸¡
pred_lm = model_lm.predict(df[['salary']])

# ì„ í˜•íšŒê·€ ê·¸ë˜í”„
sns.regplot(x=df['salary'], y=pred_lm)
```
<br><br>

## <b>ë‹¤ì¤‘ì„ í˜•íšŒê·€</b>
### <b>ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ</b>
  - ë¶„ì‚°íŒ½ì°½ ê³„ìˆ˜(VIF)ê°€ 10ì´ìƒì´ë©´ ì œê±°
  - method: patsy- dmatrices() <br>
return_typeì¸ìì— dataframeìœ¼ë¡œ ì„¤ì •ì‹œ í›„ì²˜ë¦¬ ìš©ì´
- statsmodele-variance_inflation_factor() <br>
ë¶„ì‚°íŒ½ì°½ ê³„ìˆ˜ë¥¼ ì—°ì‚°ì„ ìœ„í•´ ë°˜ë³µë¬¸ ë˜ëŠ” list comprehension ì‚¬ìš©

```python
from patsy import dmatrices
df_sub = df.loc[:,'season':'casual']
formula ='casual ~ '+ '+'.join(df_sub.columns[:-1])
y, X = dmatrices(formula , data = df_sub, return_type = 'dataframe')

#ë°˜ë³µë¬¸ ëŒë¦¬ë©´ì„œ ë³€ìˆ˜ë³„ í•˜ë‚˜ì”© ë§¤ì¹­í•˜ì—¬ ìƒê´€ê³„ìˆ˜ êµ¬í•¨
from statsmodels.stats.outliers_influence import variance_inflation_factor as via
df_vif = pd.DataFrame()
df_vif['colname'] = X.columns
df_vif['VIF'] = [vif(X.values, i) for i in range(X.shape[1])]
df_vif
#ê²°ê³¼ê°’ì—ì„œ interceptëŠ” ì ˆí¸í•­ì´ë‹ˆê¹Œ ì‹ ê²½ì•ˆì¨ë„ë¨
```

<br><br><br><br>
### <b>ëª¨ë¸í‰ê°€</b>
- R-square(ê²°ì •ê³„ìˆ˜)
- MAE
   ```python
   from sklearn.metrics import mean_absolute_error
   mean_absolute_error(yì˜ˆì¸¡ê°’, yì‹¤ì¸¡ê°’)
   ```
- MSE
   ```python
   from sklearn.metrics import mean_squared_error
   mean_squared_error(yì˜ˆì¸¡ê°’, yì‹¤ì¸¡ê°’)
   ```
- RMSE
   ```python
   mean_squared_error(yì˜ˆì¸¡ê°’, yì‹¤ì¸¡ê°’) ** 0.5
   ```
<br><br><br><br><br>
---------

<b>ì‹¤ê¸° ë¬¸ì œí’€ì´</b>
```python
import pandas as pd
import numpy as np
df = pd.read_csv('./data/DS_Sample_3.csv',encoding ='utf-8')

movie3 = df.copy()
movie3['AirDate'] = pd.to_datetime(movie3['AirDate'])

movie3['group'] = np.where(movie3['AirDate'].dt.year.isin([2005,2006,2007]),'A',
         np.where(movie3['AirDate'].dt.year.isin([2008,2009,2010]),'B','C'))
#between(2005,2007)ë„ ë¨

movie3['success'] = movie3['Rating'] * movie3['Num_Votes'] 

movie3.groupby('DirectedBy')['success'].mean().max().astype(int)

from statsmodels.stats.anova import anova_lm 
from statsmodels.formula.api import ols

model = ols(formula = 'Num_Votes~ group', data= movie3).fit()
anova_lm(model)['F'][0] + anova_lm(model)['df'][0]
```