# ML ê¸°ì´ˆ
> Machine Learning (ì¸ê³µì§€ëŠ¥) ê¸°ì´ˆí•™ìŠµ   

<br>

## Jupyter Notebook ì‚¬ìš© tip
1. shift + tab + tab : í•¨ìˆ˜ì„¤ëª…
2. Command + ì¢Œìš° >> ë¬¸ì¥ ëìœ¼ë¡œ ì´ë™ / alt + ì¢Œìš° : ë¬¸ë‹¨ ëìœ¼ë¡œ ì´ë™
3. x = í–‰ì‚­ì œ
4. z = í–‰ì‚­ì œ ì·¨ì†Œ


<br><br>

## Numpy/ Pandas ì½”ë“œ

```python
    [numpy]
 1. í•¨ìˆ˜ ë° ë©”ì„œë“œ
    type(): ìë£Œí˜• ì •ë³´ (list, array ë“±) 
    .dtype : ë°ì´í„° íƒ€ì…ì •ë³´ (int, float ë“±)
    df.select_dtypes("object") : df ë°ì´í„° ì¤‘ 'object'íƒ€ì…ì˜ ë°ì´í„°ë“¤ë§Œ ì¶”ì¶œ
    .iloc[:, df.dtypes =='int64'] : ìƒë™
    .reset_index : ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    .value_counts()
    .sort_values(by=[''])

 2. íœì‹œ ì¸ë±ì‹± (ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì¤˜ì•¼í•¨)
    df.iloc[[0,3,5,7],[3,5,7]]
    df.loc[0:3,['name','age','salary']] 
     (loc: 0-3 ë˜í•œ ìˆ«ìí˜•ì´ ì•„ë‹Œ ë¬¸ìì—´ë¡œ ì¸ë±ì‹± ëœ ê²ƒì´ë‹¤.)

 3. ë¶€ìš¸ ì¸ë±ì‹±
    df_hk.loc[df_hk['age']>30, :] #í–‰ì„ íƒì„ ì¡°ê±´ë¬¸ìœ¼ë¡œ ì¤„ìˆ˜ë„ ìˆë‹¤. 
 
 4. ì–´ë– í•œ ê°’ì´ë¼ë„ ë¬¸ìë¡œ ì²˜ë¦¬í• ë•Œ ë”°ì˜´í‘œ 3ê°œ
    '''abc'''
    """abc""" 
 
 5. a = [1,2,3], b = [4,5,6] ì¼ ë•Œ, 
    a + b = [1,2,3,4,5,6] ìœ¼ë¡œ ì¶œë ¥ëœë‹¤.
    í–‰ë ¬ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´ì„  1ì°¨ì› ë°°ì—´ë¡œ ë°”ê¿”ì¤˜ì•¼í•œë‹¤. 
    a = np.array[1,2,3] , b= array[4,5,6]
    a + b = [5,6,9]

 6. íŠœí”Œì€ í•˜ë‚˜ì˜ ì›ì†Œì¼ë•Œë„ ë’¤ì— ,ë¥¼ ê¼­! ë¶™ì—¬ì¤˜ì•¼ í•¨ (3,) ì•„ë‹ˆë©´ ìˆ«ìí˜•ìœ¼ë¡œ ì¸ì‹ë¨ (3)

 7. print ì™€ returnì˜ ì°¨ì´ì   
    returnì€ ê²°ê³¼ê°’ì„ ë˜ ê°’ìœ¼ë¡œ ë°›ì•„ì„œ ë‹¤ë¥¸ ê³³ì—ì„œ í™œìš© ê°€ëŠ¥ , í”„ë¦°íŠ¸ëŠ” ë‹¨ìˆœ ì¶œë ¥ë§Œ

 8. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   [terminal]  
      pip install tensorflow
   [jupyter notebook] \
      !pip install tensorflow
   ë˜ëŠ”
      import os
      os.system('pip install tensorflow')

 9. ë¶ˆë¦°ì¸ë±ìŠ¤ëŠ” locë§Œ ì‚¬ìš©ê°€ëŠ¥ / ilocì€ ë¶ˆê°€
   result = df_food.drop(columns=['state'])
   len(result[(result['cook_time']>0) & result['prep_time']>0])

 10. ë²”ìœ„ë¡œ ë°°ì—´ ìƒì„±
    range (ë¦¬ìŠ¤íŠ¸)
    np.arange(ë°°ì—´)
    
 11. nê°œ column ì— ëŒ€í•œ aggregation
         df_hk[['age','salary']].agg(['mean','sum','count'])
     nê°œ column ì— ëŒ€í•œ nê°œ aggregation_ê°ê° ìš”ì•½ì •ë³´ë¥¼ ë‹¤ë¥´ê²Œ ì¶œë ¥í•˜ê³  ì‹¶ì„ë•Œ ë”•ì…”ë„ˆë¦¬ ì´ìš©
     ì´ë•Œ 1ê°œ columnì— 2ê°œ ì´ìƒ aggregationì„ ì ìš©í•˜ë©´ ë§ˆì§€ë§‰ aggregationì´ ì ìš© (age : maxì ìš©ë¨)
         df_hk[['age','salary']].agg({'age':'mean', 'age':'max', 'salary':'max'})
     ë‘ê°œë¥¼ ë‹¤ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ ë°©ë²• í™œìš© (ê°ê° ì—´ì„ ì •ì˜í•˜ì—¬ ìƒì„±)
         df_hk[['age','salary']].agg(age_mean = ('age','mean'), age_max = ('age','max'), sal_cnt = ('salary','count'), sal_median = ('salary','median'))


 12. MLëŒë¦´ë•Œ í•˜ë‚˜ë¡œë„ ê²°ì¸¡ì¹˜ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ë‚˜ê²Œëœë‹¤. Merge_keyê°’ìœ¼ë¡œ ì¡°ì¸ì‹œ ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ”ì§€ í™•ì¸ì´ í•„ìš”

 13. replace ë˜ëŠ” map #dictí™œìš© 
     df_hk['gender'].replace({'F':'Female' ,'M':'Man'})

 14. Lambda
     df_hk['jumin7'].apply(lambda x : '19'+ x[0:6])

 15. ë“±ê¸‰í™” : np.where ë˜ëŠ” cut ì´ìš©

      #np.where (ì—‘ì…€ ifë¬¸ê³¼ ë™ì¼)
      np.where(df_hk['age'] < 30,'30ë¯¸ë§Œ',
            np.where(df_hk['age']<=40, '40ëŒ€','50ëŒ€ì´ìƒ'))[:10]
      # cut
      pd.cut(x = df_hk['age'], bins = [0,20,30,40,100], labels = ['10ëŒ€', '20ëŒ€', '30ëŒ€', '40ëŒ€ì´ìƒ'], right= True)
      * right = True : ì˜¤ë¥¸ìª½ ê°’ì„ í•´ë‹¹êµ¬ê°„ì— í¬í•¨í•˜ëŠ”ê²Œ T, ë‹¤ìŒ êµ¬ê°„ì— í¬í•¨í•˜ëŠ”ê²Œ F 
                       ì¦‰, ì´í•˜ì¸ì§€ ë¯¸ë§Œì¸ì§€ 
      
      binsìˆ˜ (êµ¬ê°„ì´ë‹¤ë³´ë‹ˆ) > labelìˆ˜

 16. datetime(ë‚ ì§œ)í˜•ìœ¼ë¡œ ë³€í™˜ 2ê°€ì§€ ë°©ë²•
     pd.to_datetime(df_hk['jumin7'], format = '%y%m%d-%w')
     .astype('datetime64')

     í™œìš©
      # datetime í™œìš©
      df_hk['birthday'] = df_hk['jumin7'].apply(lambda x : '19'+ x[0:6]).astype('datetime64')
      df_hk['birthday'].dt.year
      df_hk['birthday'].dt.month
      df_hk['birthday'].dt.day
      df_hk['birthday'].dt.weekday #ì›”ìš”ì¼ì€ 0
      df_hk['birthday'].dt.day_name() #day_nameì€ ()í•„ìš”
 17. Array ìƒì„±
     np.array([1,2,3])
     np.zeros((3,3))

 18. .tolist()
```
<br><br>

## Series ì™€ DataFrame
[Series] :  Pandasì˜ 1ì°¨ì›
 1) í•¨ìˆ˜ ë° ë©”ì„œë“œ
      A = pd.Series([1,2,3])
      A.values
      A.index
      A.idxmax(ìµœëŒ€ê°’ì˜ ìœ„ì¹˜)
      A.isin([1,2])

[DataFrame] : 1ì°¨ì› Seriesë¥¼ ëª¨ì•„ë†“ì€ ê²ƒ Pandasì˜ 2ì°¨ì› 
 1) í•¨ìˆ˜ ë° ë©”ì„œë“œ
   Crosstab  
    - ë‘ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì‚´í´ë³¼ë•Œ ì‚¬ìš©í•˜ëŠ” pandas í•¨ìˆ˜ (ë¹ˆë„í‘œ)
    - normalize ì¸ìì— True,1,0ê°’ì„ í• ë‹¹í•˜ì—¬ ê°’ì„ ì •ê·œí™”í• ìˆ˜ìˆìŒ
      pd.crosstab(df['aa'], df['bb'])


      pd.DataFrame([1,2,3], [4,5,6]], columns = ['A', 'B', 'C'])
      pd.DataFrame({'A':[1,4], 'B':[2,5], 'C':[3,6]})
      pd.DataFrame(np.arange(6).reshape(2,3) +1 , columns=['A','B','C'])

<br><br>

## í†µê³„
 1. mode
    ```python
    from scipy.stats import mode
    mode(array) #mode : ìµœë¹ˆê°’ (ë¹ˆë„ê°’ì´ ëª¨ë‘ ë™ì¼ í•  ë•ŒëŠ” ì²«ë²ˆì§¸ ê°’ ë¦¬í„´), count : ë¹ˆë„ìˆ˜)
    ```


 2. [ì¡°ê±´ë¶€í™•ë¥ ]ì€ crosstabì„ í™œìš©í•˜ì—¬ í‘¼ë‹¤.
    pd.crosstab(df['a'], df['b'])

 3. [ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬ ì¦ëª…]
   ```python
   import matplotlib.pyplot as plt
   import numpy as np
   import random


   avg_values = []
   for i in range (1,10000): # íšŸìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ì •ê·œë¶„í¬ë¡œ ë³€í™”
   random_sample = random.sample(range(1, 1000),100)
   x = np.mean(random_sample)
   avg_values.append(x)

   plt.hist(avg_values, bins = 100)
   plt.show()
   ```

<br><br>

## í‘œë³¸ì¶”ì¶œ

1. ë‹¨ìˆœì„ì˜ì¶”ì¶œ
2. ì¸µí™”í‘œë³¸ì¶”ì¶œ (ì˜ë£Œ-ì½”í˜¸íŠ¸ë¶„ì„)
3. ê³„í†µì¶”ì¶œ
    - ì²«í‘œë³¸ì„ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•˜ê³  í‘œì§‘ê°„ê²© kë§Œí¼ ë–¨ì–´ì§„ ê³³ì—ì„œ ë°ì´í„° ì¶”ì¶œ
4. êµ°ì§‘ì¶”ì¶œ

```Basic Code_ Random Sampling``` 
  > ìƒ˜í”Œ ì¶”ì¶œì€ ë‹¨ìˆœì„ì˜ ì¶”ì¶œì´ ê¸°ë³¸ì´ì§€ë§Œ, groupby ë¥¼ ì‚¬ìš©í•˜ë©´ ì¸µí™” í‘œë³¸ì¶”ì¶œ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ]
Sklearn - train_test_split
df_train, df_test = train_test_split(df, test_size = 0.3, random_state = 42)

1)  pandas - sample() <br>
   n= í‘œë³¸ê°œìˆ˜ frac= ë¹„ìœ¨, random_state = í‘œë³¸ì¶”ì¶œê²°ê³¼ë¥¼ ê³ ì •
   Replace : ë³µì›ì¶”ì¶œ ì—¬ë¶€, weights : ê°€ì¤‘ì¹˜
   íŒŒë¼ë¯¸í„° n ê³¼ fracì€ ë™ì‹œì‚¬ìš©ì€ ë¶ˆê°€ ```(fracì€ ë¹„ìœ¨ë¡œ ì¶”ì¶œ 0.005ë“±)``` <br>
      ```python
      df.groupby('season').sample(n=2, random_state= 42) >> ì¸µí™”ì¶”ì¶œê¸°ëŠ¥
      ```


2) scikit-learn <br>
   ```python
      from sklearn.model_selection import train_test_split

      df_train, df_test = train_test_split(df, test_size = 0.3, random_state = 42)
   ```
   <br><br>


```í‘œë³¸ ì¶”ì¶œ ë°©ë²•ë³„ ì½”ë“œ```

* ###   Random sampling
   ```python
   from sklearn.model_selection import train_test_split
   x_train, x_test = train_test_split(df_hk, test_size =0.3, random_state=42)
   x_train[:5]
   ```

* ### Sytematic sampling
  ```python
   df_idx = df_hk.reset_index()

   x_train = df_idx[(df_idx['index'] % 5 != 0)]
   x_test = df_idx[(df_idx['index'] % 5 == 0)]
   ```

* ### Cluster sampling
   ```python
   df_cluster = df_hk[df_hk['company'] =='A']
   df_cluster
   ```
* ### Stratified Random Sampling
   ```python
   df_hk['company'].value_counts()

   pd.crosstab(df_hk['company'], df_hk['gender'])
   ```
<br><br>

## ë°ì´í„° ì „ì²˜ë¦¬

   <b>í•¨ìˆ˜ ë°  ë©”ì„œë“œ</b>
   ```python
    1.  .isna(), .isnull(), .notna(), .notnull() 
      # null ê³¼ NaNê³¼ naì€ ë™ì¼í•˜ë‹¤ê³  ë´ë„ ë¬´ê´€

        df.notna().sum(axis = 1)

    2. .fillna()
       df = df.fillna(value = {'Sepal_Width':df['Sepal_Width'].mean()}) 
       ë˜ëŠ”
       df['Sepal_Width'] = df['Sepal_Width'].fillna(df['Sepal_Width'].mean())
       round(df.var(),3)

    3. .select_dtypes()
       df.select_dtypes('float64').isnull().sum().sum()


    4. Outlier (í‰ê· + 1.5 í‘œì¤€í¸ì°¨ì¼ë•Œ)
       mean = df3['Sepal.Length'].mean()
       std = df3['Sepal.Length'].std()
       len(df[(df3['Sepal.Length'] > mean+ (1.5*std))|(df3['Sepal.Length'] < mean- (1.5*std))])

    5. ì¡°ê±´ ì¶”ì¶œ numpy - where()í•¨ìˆ˜
       a = np.where(df['Sepal_Length'] >= 3)

       np.where(df_hk['age'] < 30,'30ë¯¸ë§Œ',
            np.where(df_hk['age']<=40, '40ëŒ€','50ëŒ€ì´ìƒ'))[:10]

       #ì¡°ê±´ìœ¼ë¡œ ë²”ì£¼ ë‚˜ëˆŒë•Œ npwhere, ì¡°ê±´ìœ¼ë¡œ ì¶”ì¶œì‹œ loc 
      
    6. ë³€ìˆ˜ëª… ë°”ê¾¸ê¸° 
       [íŠ¹ì • ì»¬ëŸ¼ ë³€ê²½ : pandas - .rename() ì‚¬ìš©]
       df4 = df4.rename(columns = {'is_setosa':'is_seto'})
       Dataë¡œ ì ‘ê·¼í›„ íŒŒë¼ë¯¸í„°ì—ì„œ ì»¬ëŸ¼ ì§€ì •

       [ë°ì´í„° ì „ì²´ ì¼ê´„ ë³€ê²½ : pandas - .columns ì‚¬ìš©]
       df4.columns = ['name', 'gender', 'height', 'age', 'blood_type', 'company']

    7. .astype('typeëª…')

    8. .apply(lambda x : x, )

    9. .str.slice(0:4)
       ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹±

    10. to_datetime(a)
    
      a= df5[df5['casual']>25]['datetime'] #í•„í„°ë§
      b = pd.to_datetime(a)
      b.dt.date.nunique()
      bike_time = pd.to_datetime(bike['date time'][:3])
      bike_time.dt.year
      bike_time.dt.month
      bike_time.dt.hour
      bike_time.dt.date

    11. ë”ë¯¸ë³€ìˆ˜í™”
      pd.get_dummies (data, columns= [''], drop_first =True) #get_dummiesëŠ” ê°€ë³€ìˆ˜ ëŒ€ìƒì€ ì—†ì•°
      #drop_firstë¥¼ ì‚¬ìš©í•˜ì§€ì•Šê³  ê°€ë³€ìˆ˜ë¥¼ ëª¨ë‘ í™œìš©í•  ê²½ìš°, ì™„ì „ê³µì„ ì„± ë“± ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œ ë°œìƒ
    
    12. reset_index(drop = True) : ê¸°ì¡´ì˜ ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” ë©”ì„œë“œ + ë™ì‹œì— seriesë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• ë„ í•¨ 
    (drop = True : ê¸°ì¡´ ì¸ë±ìŠ¤ ì œê±°)

    13. .set_index('ì»¬ëŸ¼ëª…') : íŠ¹ì •ë³€ìˆ˜ë¥¼ ì¸ë±ìŠ¤ë¡œ ì§€ì •í•  ê²½ìš° ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œ, ë°ì´í„° ë³‘í•© ë˜ëŠ” ì‹œê³„ì—´ ë¶„í•´ì—ì„œ ì—°ì‚°ì„ ìœ„í•´ í™œìš©

    14. ìµœëŒ€ê°’ì„ ê°€ì§„ í–‰ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬í• ë•Œ idxmax()

      [ì˜ˆì œ1]
      df5['datetime']=pd.to_datetime(df5['datetime'])
      df5['hour']= df5['datetime'].dt.hour
      df5.groupby('hour')['registered'].mean().idxmax()

      [ì˜ˆì œ2]
      a= bike[(bike['season']==2)].groupby(bike['datetime'].dt.hour)['registered'].mean()
      b= bike[(bike['season']==4)].groupby(bike['datetime'].dt.hour)['registered'].mean()
      abs(a-b).idxmax()

    15. Bindingì—°ì‚° (í–‰ ë˜ëŠ” ì—´ ì—°ì‚°ê°€ëŠ¥) : pandas- concat() 

      pd.concat([bike1, bike2.reset_index(drop = True)], axis = 1).reset_index(drop=True)

    16. Joinì—°ì‚° : pandas- merge()

      pd.merge(left = df_a, right = df_b , left_on = 'member', right_on = 'name',
            how = 'inner')
    17. crosstab() : wide form >> clustering(êµ°ì§‘í™”) ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ë¨

      pd.crosstab(dia['cut'],dia['clarity'], normalize =True).round(2)
      
      # normalize =True : ë°”ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ í™•ì¸ì´ ê°€ëŠ¥
      # normalize ëŠ” True/ False ì™¸ì— 1,0ë„ ìˆë‹¤
      # normalize = 1: ì—´ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” / 0 : í–‰ê¸°ì¤€

      #cutê³¼ clarity ë³„ë¡œ ê°€ê²©ì˜ í‰ê· 
      pd.crosstab(dia['cut'],dia['clarity'], values =dia['price'], aggfunc = pd.Series.mean)
      
      #ìœ„ì˜ ë‚´ìš©ì˜ long form ë²„ì ¼ (ê²°ê³¼ê°’ì€ ë™ì¼)
      dia.groupby(['cut', 'clarity'])['price'].mean().reset_index()
  
  18. melt()
   wide form ë°ì´í„°í”„ë ˆì„ì„ Long Formìœ¼ë¡œ ìë£Œêµ¬ì¡°ë¥¼ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ, id_vars ì¸ìì— ê¸°ì¤€ì´ ë˜ëŠ” ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì²˜ë¦¬ <br>
   
   í¬ë¡œìŠ¤íƒ­ì—ì„œ melt ì‚¬ìš© ì‹œì—ëŠ” ê¸°ì¡´ ì¸ë±ìŠ¤í™” ë˜ì–´ìˆëŠ” ì—´ì„ reset_index()ë¥¼ í†µí•´ ì—´ ë¨¼ì € ë˜ëŒë ¤ ì£¼ê³  melt í•¨ìˆ˜ ì‚¬ìš©

   elec_melt = elec.melt(id_vars= ['YEAR','MONTH','DAY'])
   elec_melt

 19. pivot()
   Long form ë°ì´í„° í”„ë ˆì„ì„ wide formìœ¼ë¡œ ìë£Œêµ¬ì¡°ë¥¼ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œ
   Index / columns/ values ì¸ìì— ê°ê° ëŒ€ìƒë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì¶œë ¥ë°ì´í„° êµ¬ì¡°ë¥¼ ê²°ì • <br>
   crosstabì˜ value + aggregate functionì™€ ê¸°ëŠ¥ì´ ë™ì¼í•˜ì§€ë§Œ, í”¼ë²—ì´ ë” ë§ì´ ì“°ì„ <br>
   ë˜í•œ êµ°ì§‘ë¶„ì„ ì‹¤ì‹œ ì „ ë°ì´í„° ì „ì²˜ë¦¬ì— ì£¼ë¡œ í™œìš©ë¨
   (pivot()ì€ pv ì‹¤í–‰ë§Œ, pivot_table()ì€ ìš”ì•½ ì—°ì‚°ê¹Œì§€)
 20. between(2005,2007) : 2005~2007 ê¹Œì§€ 
 
 21. nlargest(5) í°ìˆœì„œëŒ€ë¡œ ìƒì‰¬ 5ê°œ ì¶”ì¶œ 

 22. drop()
   
   # drop ì€ ë°ì´í„° ë‹¨ìœ„ë¡œ ì ‘ê·¼ 
   # 1. dropnaì€ subset ì´ìš©
     df_hk_na= df_hk_na.dropna(subset ='age')

   # 2. drop(row index ì‚¬ìš©)ìœ¼ë¡œ ì‚­ì œ
     df_hk.drop(index = df_hk[(df_hk['expenditure'] < min) | (df_hk['expenditure'] > max)].index
  
   # 2-1(ì¤‘ìš”!!). drop ëŒ€ì‹  ë°˜ëŒ€ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§ ê°€ëŠ¥ 
     df_hk[(df_hk['expenditure'] >= min) & (df_hk['expenditure'] <= max)]
     
     [ì°¸ê³ ] (í–‰ì¶”ì¶œì‹œ 'NOT'ì€ ~ ë¡œ ì‚¬ìš© ê°€ëŠ¥)
     df_ds = df_ds[(~df_ds['last_new_job'].isin(['>4', 'never']))]

   # 3. ê·¸ì™¸ columns ì‚¬ìš©í•˜ì—¬ ì‚­ì œ ê°€ëŠ¥

   23. .pow(2) : ì§€ìˆ˜ì—°ì‚° (2ì¼ë•ŒëŠ” ì œê³±)
        ** ì™€ ë™ì¼ (**0.5 ëŠ” ë£¨íŠ¸ê°€ ëœë‹¤)

   ```

 
     

  
   <br><br>

 ## ë°ì´í„° ì‹œê°í™”

 ```import matplotlib.pyplot as plt``` <br>
 ```import seaborn as sns ```<br><br>

 ## #1. histogram 

<b>[matplotlib]</b> <br>
ì†ì„±ê°’ ì°¸ê³  : https://matplotlib.org/stable/api/markers_api.html
```python
figure = plt.figure(figsize=(5, 4), facecolor ='pink')
plt.plot([0,1,2,3],[1,4,9,12], color = 'red', marker = 'o', markersize = 3,
         linestyle ='dashed', linewidth = 0.5)
plt.plot([0,1,2,3],[10,20,30,40], color = 'gray', marker = '', markersize = 8,
         linestyle ='solid', linewidth = 0.5)

plt.xlabel('x_label')
plt.ylabel('y_label')
plt.title('matplotlib chart')


plt.show()
```
<br><br>
<b>[seaborn]</b> <br>

* seabornì˜ histogramì€ histplotê³¼ displotì´ ëŒ€í‘œì ì´ë©° histplotì€ axesë ˆë²¨, displotì€ figureë ˆë²¨ì„
```python
# seaborn histogram histplot
figure = plt.figure(figsize = (3,2))
sns.histplot(df_hk['age'], kde=True) 
#kde:ì¶”ì²­ì„  ë³´ì—¬ì£¼ê¸°
sns.histplot(x='age', data=df_hk, kde=True, bins=30)
```
 <br>

## #2. distplot

<b>[seaborn]</b> <br>
   ```python
   # seaborn histogram distplot í™•ë¥ ë°€ë„í•¨ìˆ˜(ì¶”ì •ì„ )
figure = plt.figure(figsize = (3,2))
sns.distplot(df_hk['age'])
```
 <br>

## #3. countplot
<b>[seaborn]</b> <br>
   ```python
plt.figure(figsize=(4, 3))
sns.countplot(x='company', data=df_hk)
```
 <br>

## #4. barplot
<b>[matplotlib]</b> <br>
```python
# bar ê°’ì„ ë§Œë“¤ì–´ì•¼ í•¨

plt.figure(figsize=(4, 3))
a_mean = df_hk[df_hk['company'] == 'A'].salary.mean()
b_mean = df_hk[df_hk['company'] == 'B'].salary.mean()
c_mean = df_hk[df_hk['company'] == 'C'].salary.mean()
X = df_hk['company'].unique()

plt.bar(x=X, height=[a_mean,b_mean,c_mean])
plt.xticks([0,1,2],['company A','company B','company C']) # plt.xticks([0,1,2] ëˆˆê¸ˆê°„ê²©
plt.show()
```
<b>[seaborn]</b> <br>
* seaborn barplot yê°’ default meanìœ¼ë¡œ ê³„ì‚° (sum, median ë³€ê²½ ê°€ëŠ¥)

```python
plt.figure(figsize=(3,2))
sns.barplot(data = df_hk, x= 'company', y= 'salary')


#confidence intervalì„ ì—†ì• ê³ , colorë¥¼ greenìœ¼ë¡œ í†µì¼, í‰ê· ì™¸ì— ì´í•©/ ì¤‘ì•™ê°’ìœ¼ë¡œ í‘œí˜„. 
# hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™” (ë‚¨/ì—¬ ë°ì´í„°ë¡œ í•œë²ˆ ë” ë‚˜ëˆ„ê¸°) #x,yì¶• ë°ì´í„° ë°”ê¿”ì£¼ë©´ ì¶• ë°”ê¿”ì„œ ê·¸ë ¤ì¤Œ
# estimator= np.median, np.sum

plt.figure(figsize=(3,2))
sns.barplot(data = df_hk, x= 'company', y= 'salary', estimator='sum', ci=None ,color='green', hue='gender')

```
## #5. boxplot
<b>[matplotlib]</b> <br>
```python
# matplotlib boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)
plt.figure(figsize=(3,4))
plt.boxplot(x= df_hk[['age','height']],data=df_hk)
plt.show

# matplotlib boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)
a_age = df_hk[df_hk['company']=='A']['age']
b_age = df_hk[df_hk['company']=='B']['age']
c_age = df_hk[df_hk['company']=='C']['age']

plt.figure(figsize=(3,4))
plt.boxplot([a_age, b_age, c_age])
plt.show
```

<b>[seaborn]</b> <br>
```python
# seaborn boxplot, x(ë²”ì£¼í˜•), y(ì—°ì†í˜•)ì— ëŒ€í•œ 4ë¶„ìœ„ê°’ì„ í‘œí˜„
plt.figure(figsize=(3,4))
sns.boxplot(data=df_hk, x= 'company' ,y='age')

# hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™”
plt.figure(figsize=(3,4))
sns.boxplot(data=df_hk, y= 'company' ,x='age', hue='gender')
```
<br>

## #6. Pie Chart
<b>[matplotlib]</b> <br>
```python
plt.figure(figsize=(3,4))
a_count = df_hk[df_hk['company'] == 'A'].company.count()
b_count = df_hk[df_hk['company'] == 'B'].company.count()
c_count = df_hk[df_hk['company'] == 'C'].company.count()

plt.pie(x = ([a_count,b_count,c_count]), 
        labels=(['company A','company B','company C']), autopct='%.1f%%') # autopct ì „ì²´ ë°±ë¶„ìœ¨, ' %.2f 'ëŠ” ì†Œìˆ«ì  2ìë¦¬

plt.show()
```
## #7. Scatter Plot
<b>[seaborn]</b> <br>

```python
#hue ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ xê°’ ì„¸ë¶„í™”
sns.scatterplot(df_hk, x= 'age', y='salary',hue='company')
```
<b>[python-plot]</b> <br>
```python
plt.figure(figsize=(3,4))
df_hk.plot.scatter(x= 'age' , y='salary')
```

## #8. heatmap
<b>[seaborn]</b> <br>
```python
#DataFrameì˜ corr()ì€ ìˆ«ìí˜• ê°’ë§Œ ìƒê´€ë„ë¥¼ êµ¬í•¨> ìƒê´€ë„ë¥¼ ë¨¼ì € êµ¬í•´ì•¼ë¨

# annotation(ì£¼ì„) ì¸ìë¡œ ìƒê´€ê³„ìˆ˜ í‘œì‹œ
plt.figure(figsize=(4,4))
corr=  df_hk.corr()
sns.heatmap(corr, annot = True)
#DataFrameì˜ corr()ì€ ìˆ«ìí˜• ê°’ë§Œ ìƒê´€ë„ë¥¼ êµ¬í•¨

```
<br><br>

# ê°€ì„¤ê²€ì •

## <b>T-TEST ê²€ì •</b>
> ë‘ ì§‘ë‹¨ê°„ì˜ í‰ê· ë¹„êµ

<br><br>

<b>1.  t_test_1sample</b>

* t-testë¥¼ í•  dataì˜ mean ê·¼ì²˜ì˜ ê°’ìœ¼ë¡œ t-testí›„ tí†µê³„ëŸ‰ê³¼ p_value ê´€ì°° #ëª¨ì§‘ë‹¨ í‰ê· ì„ ì•Œê³  ìˆìŒ
* <b> popmean: ëª¨ì§‘ë‹¨ì˜ ì¶”ì •ëª¨ìˆ˜  ì¦‰, ğ’â‚</b> <br>
*  ëª¨í‰ê· ê³¼ ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ í†µê³„ëŸ‰ì€ ë‚®ì•„ì§€ê³  p-valueê°€ ì˜¬ë¼ê°„ë‹¤. <br>
*  Scipy -ttest_1samp() ì‚¬ìš© <br>
```í™œìš©: ê°ë‹¨ê°€ í‰ê· ì´ 00ë¼ê³  ì•Œë ¤ì ¸ìˆë‹¤. ì‚¬ì‹¤ì¸ê°€?```
  
```python 
from scipy.stats import ttest_1samp
ttest_1samp(df_hk['age'], popmean = 39.24)[1] < 0.05  

# ê²°ê³¼ê°’ : TtestResult(statistic=0.0, pvalue=1.0, df=249)
1.0 < 0.05
# ê²°ê³¼ : False, ê²°ê³¼ í•´ì„: 95% ì‹ ë¢°ìˆ˜ì¤€ìœ¼ë¡œ 100% ì¼ì¹˜
```

<br><br>
<b>2. Two sample t-test</b>

* ë‹¤ë¥¸í™˜ê²½ (ì£¼ë§/ì£¼ì¤‘) ì¦‰, ë…ë¦½ëœ ëª¨ì§‘ë‹¨ì—ì„œ ì¶”ì¶œëœ ê° ë‘ ì§‘ë‹¨ê°„ í‰ê· ì´ ê°™ì€ì§€ ê²€ì •
* ë“±ë¶„ì‚° ì—¬ë¶€ì—ë”°ë¼ ê²€ì •í†µê³„ëŸ‰ ê³„ì‚°ì‹ì´ ë‹¬ë¼ì„œ ë“±ë¶„ì‚° ê²€ì •ì„ í•´ì¤˜ì•¼í•œë‹¤.
* ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° wilcoxon rank sum test (ìˆœìœ„í•©ê²€ì •) ì‚¬ìš©
* ë“±ë¶„ì‚° ê°€ì •ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°, equal_var ì¸ìì— Trueë¥¼ í• ë‹¹
* ttest_ind() ì‚¬ìš© <br>
```í™œìš©: ì£¼ì¤‘ ê°ë‹¨ê°€ì™€ ì£¼ë§ ê°ë‹¨ê°€ê°€ ê°™ì€ê°€?```

```python
from scipy.stats import ttest_ind
ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary)

#ê²°ê³¼ : Ttest_indResult(statistic=5.941362455469809, pvalue=1.2532322871358408e-08)

```


2-1.  sample t-test (A>=B) #less_ A(ğ’â‚€)ë³´ë‹¤ B(ğ’â‚)ê°€ ì‘ë‹¤ (í•˜ë‹¨ì¸¡ê²€ì •)
   ```python
   ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary,
         alternative='less')

   #ê²°ê³¼ê°’: Ttest_indResult(statistic=5.941362455469809, pvalue=0.9999999937338386)
   ```

2-2. sample t-test (A<=B) #greater_ A(ğ’â‚€)ë³´ë‹¤ B(ğ’â‚) ê°€ í¬ë‹¤(ìƒë‹¨ì¸¡ê²€ì •)
   ```python
   ttest_ind(df_hk[(df_hk['company']=='A')].salary , df_hk[(df_hk['company']=='B')].salary,
            alternative='greater')

   #Ttest_indResult(statistic=5.941362455469809, pvalue=6.266161435679204e-09)
 ```

 > ì˜ˆì œ 
 1) iris ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬('iris.csv') species column 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14ì™€ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ì‹œì˜¤
      ```python
      # H0 : 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14 ì™€ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤ (ìœ ì˜ìˆ˜ì¤€ 0.05)
      # H1 : 'virginica'ì˜ 'sepal_width' ëª¨í‰ê· ì´ 3.14 ì™€ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ì—†ë‹¤ (ìœ ì˜ìˆ˜ì¤€ 0.05)
      #df_iris['sepal_width'].mean()
      from scipy.stats import ttest_1samp, ttest_ind

      ttest_1samp(df_iris['sepal_width'], popmean = 3.14)[1] < 0.05
      #ê²°ê³¼ê°’:  True = H1 ì§€ì§€, H0 ê¸°ê°
      ```
 2) 'setosa'ì™€ 'versicolor'ì˜ sepal_length í‰ê· ì´ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ì‹œì˜¤
      ```python
      from scipy.stats import ttest_1samp, ttest_ind
      a = df_iris[(df_iris['species']=='setosa')]['sepal_length']
      b = df_iris[(df_iris['species']=='versicolor')]['sepal_length']

      ttest_ind(a,b)[1] <0.05
      #ê²°ê³¼ê°’ : True = H1 ì§€ì§€, H0 ê¸°ê°
      ```
<br>
<b>3. Paired sample t-test</b>

 * ë™ì‹œê°„ëŒ€ ë“± í™˜ê²½ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì¤€ ë™ì¼í•œ ëª¨ì§‘ë‹¨ì—ì„œë¶€í„° ì¶”ì¶œëœ  ë‘ ì§‘ë‹¨ê°„ ì°¨ì´ê°€ ìˆëŠ”ì§€ ê²€ì •
 * í‘œë³¸ì´ ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš° wilcoxon rank sum test (ìˆœìœ„í•©ê²€ì •) ì‚¬ìš©
 * Scipy -ttest_rel() ì‚¬ìš©
 * ```í™œìš©: ë™ë…„, ë™ì›”, ë™ì‹œê°„ëŒ€ ì´ìš©ê° ëŒ€ìƒìœ¼ë¡œ ë¹„íšŒì›ì˜ ëŒ€ì—¬ëŸ‰ê³¼ íšŒì›ì˜ ìì „ê±° ëŒ€ì—¬ëŸ‰ì˜ í‰ê· ì°¨ì´ ê²€ì •```



<br><br><br>

## <b>ANOVA ê²€ì •</b> 

* ìˆ˜ì¹˜ê°€ í†µê³„ì ìœ¼ë¡œ ë™ì§ˆì ì¸ì§€ ì´ì§ˆì ì¸ì§€ ê²€ì¦í•˜ê¸° ìœ„í•´ í˜„ì—…ì—ì„œ ë§ì´ ì‚¬ìš©ë¨ <br>
* ê·¸ë£¹ì´ 2ê°œ ì´ìƒ ì¼ë•Œ ì‚¬ìš©í•˜ë©°, 2ê°œì¸ ê²½ìš°ëŠ” t-testì˜ ê²°ê³¼ê°’ê³¼ ë™ì¼í•˜ê²Œ ë‚˜ì˜¨ë‹¤. <br>

<b> 1. ì¼ì›ë¶„ì‚°ë¶„ì„ (One way Anova)</b>
* ì„¸ì§‘ë‹¨ ì´ìƒ ê°„ì˜ í‰ê· ì°¨ì´
* ìˆ˜ì¹˜í˜• ì¢…ì†ë³€ìˆ˜(í‰ê· )ì™€ ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ ê°ê°1í•˜ë‚˜ì”©ì¼ë•Œ ì‹¤ì‹œ
* (ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ ì˜ˆì‹œ: company ê°™ì€ ì§‘ë‹¨)
* ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ ë¬¸ìí˜•ì¸ê²½ìš°ì—ëŠ” ìƒê´€ì—†ì§€ë§Œ, 
  1,2,3,4 ì²˜ëŸ¼ êµ¬ë¶„ë˜ì–´ìˆëŠ” ê²½ìš° ìˆ«ìí˜•ìœ¼ë¡œ ì¸ì‹í•˜ê²Œí•˜ì§€ ì•Šê¸° ìœ„í•´, ì¼ë°˜ì ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ ì•ì— Cë¡œ ê°ì‹¸ì¤€ë‹¤.
   ```python
   model= ols(formula = 'price ~ C(color)', data = df).fit()
   anova_lm(model)
   ```
  ```í™œìš©: ê¸ˆ,í† ,ì¼ì´ ê°™ì€ 'ì£¼ë§'ì¸ì§€ or 80ì ê³¼ 81ì ì´ ë™ì¼í•œ ìˆ˜ì¤€ì¸ì§€ ë“±```

<br>

<b>[scipy]</b> <br>
```python
# ANOVA scipy.stats ì‚¬ìš©
from scipy.stats import f_oneway

##sample test ì¤€ë¹„í•„ìš”
a= df_hk[df_hk['company'] =='A'].salary
b= df_hk[df_hk['company'] =='B'].salary
c= df_hk[df_hk['company'] =='C'].salary

f_oneway(a,b,c)[1] < 0.05 #H0ê¸°ê° :íšŒì‚¬ì™€ ì—°ë´‰ì€ ì—°ê´€ì´ ìˆë‹¤.
```
<b>[statsmodels]</b> #ë¶„ì‚°ë¶„ì„í‘œ ì œê³µ
```python
# ANOVA statsmodels ì‚¬ìš©
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

model = ols(formula = 'salary~ company', data= df_hk).fit() #formula: ì¢…ì†ë³€ìˆ˜~ë…ë¦½ë³€ìˆ˜ 

anova_lm(model)
```
<br><br>
<b>2. ì´ì›ë¶„ì‚°ë¶„ì„(Two way Anova)</b>
* ìˆ˜ì¹˜í˜• ì¢…ì†ë³€ìˆ˜ 1ê°œ, ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œì¼ë•Œ ì‹¤ì‹œ
* ì£¼ìš”íš¨ê³¼ + êµíš¨íš¨ê³¼ (ë…ë¦½ë³€ìˆ˜ê°™ì˜ ì˜í–¥ì„±) í•¨ê»˜ ë¶„ì„
* ì£¼ìš”íš¨ê³¼ ê·€ë¬´ê°€ì„¤ : í‰ê· ì´ ê°™ìŒ
* êµí˜¸ì‘ìš©íš¨ê³¼ ê·€ë¬´ê°€ì„¤:  ìš”ì¸ê°„ êµí˜¸ì‘ìš©ì´ ì—†ìŒ 
* : ë¥¼ ì´ìš© (ì£¼ìš”íš¨ê³¼ ë’¤ +ë¡œ ì—°ê²°)

```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
model = ols(formula = 'price ~ cut + color + cut:color', data=df).fit()
anova_lm(model)

#ê²°ê³¼í•´ì„: ë‹¤ì´ì•„ëª¬ë“œì˜ cutë³„ ë˜ëŠ” colorë³„ê°„ì— price í‰ê· ì´ ê°™ìœ¼ë©°(ì°¨ì´ ì—†ìŒ), cut-colorê°„ êµí˜¸ì‘ìš©ë„ ì—†ë‹¤.
```
```python
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

model = ols('registered ~ C(season)+C(holiday) + C(season):C(holiday)',
           data= df_bike).fit()
anova_lm(model).round(5)
```








## <b>ì‚¬í›„ê²€ì •_tukey</b> 
* ë…ë¦½2í‘œë³¸ t-ê²€ì •ê³¼ ìœ ì‚¬
* reject : ìœ ì˜ìˆ˜ì¤€ ì•ˆì—ì„œ 'ê·€ë¬´ê°€ì„¤' ê¸°ê° ì—¬ë¶€ 
  (False : ê¸°ê° ë¶ˆê°€ /  True : reject (ê¸°ê°))
* True : ê·€ë¬´ê°€ì„¤ ê¸°ê°ì´ë‹ˆê¹Œ ë‘ ì§‘ë‹¨ì€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒ.
  
```python
#endog : y label
#alpha : ìœ ì˜ ìˆ˜ì¤€ 0.05
from statsmodels.stats.multicomp import pairwise_tukeyhsd

posthoc = pairwise_tukeyhsd(df_hk['salary'], df_hk['company'], alpha =0.05 ) #ì¢…ì†,ë…ë¦½ ìˆœ

print(posthoc) #ë³€ìˆ˜ì— í• ë‹¹í•´ì„œ í”„ë¦°íŠ¸ í•„ìš”
```


<br>

> ì˜ˆì œ

bike ë°ì´í„°(bike.cvs)ë¥¼ ì‚¬ìš©í•˜ì—¬, ìš”ì¼ë³„ registered í‰ê· ì´ ê°™ì€ì§€ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ê²€ì •í•˜ë‹ˆ,

í‰ê· ì´ ê°™ì§€ ì•Šì„ë•Œ, í‰ê· ì´ ìœ ì˜ìˆ˜ì¤€ 0.05ì—ì„œ ì°¨ì´ë‚˜ì§€ ì•ŠëŠ” ì¡°í•©(False)ì€ ëª‡ ê°œì¸ê°€ ?
```python
from statsmodels.stats.multicomp import pairwise_tukeyhsd

post_hoc = pairwise_tukeyhsd(df_bike['registered'], df_bike['date'], alpha = 0.05)
print(post_hoc)
```

<br>

## <b>ìƒê´€ë¶„ì„</b>
- ìƒê´€ê³„ìˆ˜ë¥¼ í†µí•´ ì„ í˜•ê´€ê³„ ì—¬ë¶€ë¥¼ ë¶„ì„
- Pandas-corr()ë©”ì„œë“œ : ìƒê´€ê³„ìˆ˜ë§Œ ì œê³µ
- Scipy í•¨ìˆ˜ : ìƒê´€ê³„ìˆ˜ì™€ p-value í™•ì¸ ê°€ëŠ¥ <br>
    (ê²€ì •í†µê³„ëŸ‰ì„ ë‚´ë¶€ì ìœ¼ë¡œ ê°€ì§€ê³  ìˆì§€ë§Œ ë³´ì—¬ì§€ëŠ” ìˆ«ìëŠ” ê²€ì •í†µê³„ëŸ‰ì´ ì•„ë‹Œ ìƒê´€ê³„ìˆ˜ì™€ PV)
- ê²€ì • í†µê³„ëŸ‰ : t
- ê±°ì˜ pearsonì´ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, spearnam,kendallì˜ ìˆœì„œí˜• ìƒê´€ë¶„ì„ì€ ëŒ€í‘œì ìœ¼ë¡œ ì„¤ë¬¸ì¡°ì‚¬ (ë‚˜ì¨/ ì¢‹ìŒ /ë§¤ìš°ì¢‹ìŒ ë“±)ì— ì‚¬ìš©ë¨
  



<b>[corr()]</b> <br>
```python
# pearsonr, spearmanr, kendalltau
df_hk.corr() #default method = pearson
df_hk.corr(method='pearson')
df_hk.corr(method='kendall')
df_hk.corr(method='spearman')

#groupbyë¥¼ í™œìš©í•œ ìƒê´€ë¶„ì„
df2= df.groupby('weather')[['temp','casual']].corr()
df2 = df2.reset_index()
df2[(df2['atemp'])<1]

```
<b>[scipy]</b> <br>
```python
# pearsonr
from scipy.stats import pearsonr
pearsonr(df_hk['age'], df_hk['salary'])

# spearmanr
from scipy.stats import spearmanr
spearmanr(df_hk['age'], df_hk['salary'])

# kendalltau
from scipy.stats import kendalltau
kendalltau(df_hk['age'], df_hk['salary'])
```


> ì˜ˆì œ 
 <br>
1.
   temp, atemp, humidity, registeredì˜ ìƒê´€ ê³„ìˆ˜ì¤‘ ê°€ì¥ ë†’ì€ê²ƒì€ ?
   ```python
   df_bike[['temp','atemp','humidity','registered']].corr()
   ``` 
<br>

1. ë‚ ì”¨ê°€ ë§‘ì€ë‚ (weather = 1) ê³¼ ê·¸ë ‡ì§€ ì•Šì€ë‚  ì˜¨ë„(temp)ì™€ ìì „ê±° ëŒ€ì—¬ ìˆ«ì(casual)ì˜ ìƒê´€ê³„ìˆ˜ì˜ ì ˆëŒ€ê°’ì€ ì–¼ë§ˆì¸ê°€ ?
      ```python
      df_bike[(df_bike['weather'] == 1)][['temp','casual']].corr()
      ```
```!! corrëŠ” DFë¡œ ì¶”ì¶œí•´ì•¼í•¨ [[]] !!```


<br>

## <b>ë…ë¦½ì„±(ì—°ê´€ì„±) ê²€ì •</b> ```[ì¹´ì´ì œê³±ê²€ì •]```

   - ì í•©ë„, ë…ë¦½ì„±, ë™ì§ˆì„± ê²€ì • ì‚¬ì „ì— ì§„í–‰ í•„ìš”
   - ê²€ì •í†µê³„ëŸ‰ : ì¹´ì´ì œê³± âˆ‘ ((ê´€ì¸¡ë„ìˆ˜ - ê¸°ëŒ€ë„ìˆ˜)Â² / ê¸°ëŒ€ë„ìˆ˜)
   - crosstab (ë¹ˆë„í‘œ) ìë£Œ í™œìš© í•„ìš”
   - ë‘ ëª…ëª©í˜• ìë£Œ ê²€ì • <br>
     (ìˆ˜ì¹˜í˜•ì¼ ê²½ìš°ëŠ” êµ¬ê°„í™”/ë²”ì£¼í™”ë¥¼ í†µí•´ì„œ ëª…ëª©í˜•ìœ¼ë¡œ ë³€í™˜í›„ ì‚¬ìš©)
   - correction = False ëŠ” ì—°ì†ì„± ìˆ˜ì •ì„ ì ìš©í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•¨ ```chi2_contingency(crosstab2 , correction = False)```
 <br>

```python
# ê´€ì¸¡ê°’ cross ì œê³µ í•„ìš”

cross = pd.crosstab(df_hk['gender'], df_hk['company'])
#margins = True : í–‰/ì—´í•©ê³„, normalize = True: ì „ì²´ê¸°ì¤€

from scipy.stats import chi2_contingency #ë…ë¦½ì„±ê²€ì •
chi2_contingency(cross)

#ê²°ê³¼ê°’ dof =ììœ ë„
Chi2ContingencyResult(statistic=1.674107142857143, pvalue=0.43298440342651534, dof=2, expected_freq=array([[44.8, 44.8, 22.4],
       [55.2, 55.2, 27.6]

pvalue=0.43298440342651534 > 0.05
#H0 ì±„íƒ: ë…ë¦½ì´ë‹¤(ìƒê´€ì—†ë‹¤)
```

> ì˜ˆì œ

1) seasonê³¼ weather dtypeì„ ë¬¸ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
ë‘ ë³€ìˆ˜ê°€ ê´€ë ¨ìˆëŠ”ì§€ ì ì ˆí•œ ê²€ì •ì„ í•˜ê³  ê²€ì •í†µê³„ëŸ‰ê³¼ p-valueë¥¼ êµ¬í•˜ì‹œì˜¤
   ```python
   df_bike['weather']= df_bike['weather'].astype('str')
   df_bike['season']= df_bike['season'].astype('str')
   cross = pd.crosstab(df_bike['weather'], df_bike['season'])
   chi2_contingency(cross)[1] < 0.05
   #h0 ê¸°ê°
   ```

2) ìì „ê±° ì´ ëŒ€ì—¬ìˆ˜(count)ê°€ ìƒìœ„ 30%ì¼ë•Œ 'high', ê·¸ ë¯¸ë§Œ ì¼ë•Œ 'low' ì¸ íŒŒìƒë³€ìˆ˜(count_high)ë¥¼ ìƒì„±í•˜ê³ 
count_highì™€ workingdayì˜ ì—°ê´€ì„± ì—¬ë¶€ë¥¼ ê²€ì •í•˜ê³  ê²€ì • í†µê³„ëŸ‰ì„ êµ¬í•˜ì‹œì˜¤ (ì†Œìˆ«ì  ë„·ì§¸ìë¦¬ ë°˜ì˜¬ë¦¼í•˜ì—¬ í‘œê¸°)
   ```python
   import numpy as np
   df_bike['workingday']= df_bike['workingday'].astype('str')

   df_bike['count_high'] = np.where(df_bike['count']>= df_bike['count'].quantile(0.3), 'high','low')
   cross = pd.crosstab(df_bike['count_high'], df_bike['workingday'])
   chi2_contingency(cross)[0].round(4)
   ```


<br><br>

## ë“±ë¶„ì‚° ê²€ì •
* ë‘ ì§‘ë‹¨ ë˜ëŠ” ê·¸ì´ìƒì˜ ì§‘ë‹¨ê°„ ë¶„ì‚°ì´ ê°™ì€ì§€ ì—¬ë¶€ ê²€ì •
* ê·€ë¬´ê°€ì„¤ : ì§‘ë‹¨ê°„ ë¶„ì‚°ì€ ì„œë¡œ ê°™ìŒ

  1. f_test : ë‘ì§‘ë‹¨ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì‚¬ìš© 
     (Anova ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„)
      
      <b>pythonì—ëŠ” fê²€ì • í•¨ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì—
      Scipy - f.cdf() (ëˆ„ì ë¶„í¬ê´€ë ¨ í•¨ìˆ˜)ì—
      f ê²€ì •í†µê³„ëŸ‰ (ë¶„ì‚°ë¹„), ì²«ë²ˆì§¸ ë°ì´í„°ì˜ ììœ ë„, ë‘ë²ˆì§¸ ë°ì´í„°ì˜ ììœ ë„ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì—¬ p-valueë¥¼ êµ¬í•œë‹¤.</b>
  2. Bartlett's test : ë‘ì§‘ë‹¨ ì´ìƒ ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì‚¬ìš©
  3. Lavene's test : ë‘ì§‘ë‹¨ ì´ìƒ ëŒ€ìƒ / ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ í•„ìš”ê°€ ì—†ìŒ

> ì˜ˆì œ

1) ë‚¨ì„±ê³¼ ì—¬ì„±ì˜ 1íšŒ í‰ê·  ì†¡ê¸ˆì•¡ì˜ ë¶„ì‚°ì„ ë¹„êµê²€ì •í•˜ê³ , ê·¸ ê²°ê³¼ì˜ ê²€ì •í†µê³„ëŸ‰ì€ ì–¼ë§ˆì¸ê°€?
(2ì§‘ë‹¨ì´ë‹ˆê¹Œ ë¶„ì‚°ë¹„ë¥¼ êµ¬í•´ì„œ Fê²€ì •ë„ ê°€ëŠ¥)



   ```python
   from scipy.stats import f
   from scipy.stats import bartlett
   from scipy.stats import levene

   #ê³µí†µì „ì²˜ë¦¬
   df['trans_mean'] = df['Total_trans_amt'] / df['Total_trans_cnt']
   M_a =df[(df['Gender']== 'M')]['trans_mean']
   F_a =df[(df['Gender']== 'F')]['trans_mean']

   #---Fê²€ì •---

   #Fí†µê³„ëŸ‰
   F = M_a.var() / F_a.var()
   print(F)

   #fê²€ì •_cdf
   result = f.cdf(F, dfd = len(M_a)-1, dfn = len(F_a)) 
   print(result)

   #p-value
   p = (1-result) * 2


   #---bartlett ê²€ì •---
   bartlett(M_a, F_a)


   #---levene ê²€ì •---
   levene(M_a, F_a)
   ```

<br><br>

## ì‹œê³„ì—´ë¶„ì„
### <b>í‰í™œí™”</b>
 - ì´ë™í‰ê· ë²• 
   -  ë‹¨ìˆœì´ë™í‰ê· ë²• (Simple Moving Average)
  
      ë©”ì„œë“œ : Pandas =rolling() <br>
      windowì—ëŠ” ì´ë™í‰ê· ëŒ€ìƒì´ ë˜ëŠ” ë°ì´í„° ê°œìˆ˜ n ë¥¼ ì§€ì • <br>
      nì¼ë¶€í„° í‰ê· ì¹˜ ê³„ì‚°ë¨ìœ¼ë¡œ ìµœì´ˆë°ì´í„° n-1ê°œ ë§Œí¼ ê²°ì¸¡ì¹˜ ì¡´ì¬ <br>
      centerì— Trueë¥¼ ì…ë ¥í•  ê²½ìš° ì¤‘ì‹¬ ì´ë™ í‰ê· ì‹¤ì‹œ ê°€ëŠ¥ <br>

      ```python
      import pandas as pd
      df = pd.read_csv('./data/seoul_subway.csv')

      df_sub["mean5"] =df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'].rolling(window=5).mean()
      df_sub[:10]
      ```


   -  ê°€ì¤‘ì´ë™í‰ê· ë²• (Weighted Moving Average)
  <br><br>

 - ì§€ìˆ˜í‰í™œë²• 
   -  ë‹¨ìˆœì§€ìˆ˜í‰í™œë²•(EWMA)  
      ë©”ì„œë“œ : Pandas-ewm() <br> 
      Alpha = ì§€ìˆ˜í‰í™œê³„ìˆ˜ ì…ë ¥
      ```python
      df_sub["EWMA"] =df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'].ewm(alpha = 0.9).mean()
      df_sub[:10]
      ```


   -  ì´ì¤‘ì§€ìˆ˜í‰í™œë²•(Winters) 
   -  ì‚¼ì¤‘ì§€ìˆ˜í‰í™œë²•(HoltWinters)


### <b>ì‹œê³„ì—´ë¶„í•´</b>


- ë©”ì„œë“œ : statsmodels - seasonal_decompose()
- Model ì¸ìì— 'multiplicative'ë¥¼ ì…ë ¥í•˜ë©´ ìŠ¹ë²•ëª¨í˜• ì ìš©(ê¸°ë³¸ = ê°€ë²•ëª¨í˜•
- :star: ì‹œê³„ì—´ ë°ì´í„° ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤í™” í•´ì¤˜ì•¼í•œë‹¤.

```python
from statsmodels.tsa.seasonal import seasonal_decompose
#tsa : time series analysis

df['ì‚¬ìš©ì¼ì'] = pd.to_datetime(df['ì‚¬ìš©ì¼ì'], format = '%Y%m%d')
df_sub = df[(df['ë…¸ì„ ëª…'] =='1í˜¸ì„ ')&(df['ì—­ëª…'] =='ì¢…ë¡œ3ê°€')]
df_sub = df_sub.set_index('ì‚¬ìš©ì¼ì') #ì‹œê³„ì—´ ë°ì´í„° ì¸ë±ìŠ¤í™” : ë°ì´í„°ë ˆë²¨ë¡œ ì ‘ê·¼

result = seasonal_decompose(df_sub['ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜'][:200])
result.plot()
#result.seasonal
#result.trend 
#result.resid 
```


<br><br>

# <b>Machine Learning </b>

## <b>1. Pre-processing</b>
### <b>1. Scaling</b>

 - ìŠ¤ì¼€ì¼ë§ ì „í›„ ë¹„êµë¥¼ ìœ„í•´ histogram 2ê°€ì§€

   ```python
   df_hk.hist()

   sns.histplot(df_hk[['height','age', 'salary' , 'expenditure']])
   ```



 -  min_max scaling (ìµœì†Œ-ìµœëŒ€ ë³€í™˜) (ë²”ìœ„:0~1) <br>
    -  í‘œì¤€í™” : min-max ë‹¨ìœ„ë¥¼ ê³ ë¥´ê²Œ í•˜ê¸° ìœ„í•˜ì—¬ ëª¨ë“  ê°’ì„ 0~1ì‚¬ì´ë¡œ ë°”ê¾¸ëŠ” ê²ƒ 
      ```python
      # ëŒ€ìƒë³€ìˆ˜ ì„ íƒ (ìˆ˜ì¹˜í˜•) 'height', 'age', 'salary', 'expenditure'

      df_num = df.select_dtypes(include=['float64','int64'])
      df_cat = df.select_dtypes(include=['object'])


      from sklearn.preprocessing import MinMaxScaler
      scaler = MinMaxScaler()
      scaler.fit(df_num)
      df_scaled = scaler.transform(df_num)
      pd.DataFrame(df_scaled, columns = df_num.columns)

      # ì‹œê°í™” 

      fig, ax = plt.subplots( nrows= 1 , ncols=2, figsize=(14, 5))

      ax[0].set_title('original ')
      ax[1].set_title('minmax')


      df_num.plot.hist(ax= ax[0] )
      df_mm.plot.hist(ax= ax[1] )
      plt.show()

      ```

 -  standard scaling (Z-score ë³€í™˜) (ë²”ìœ„ : 0 ì¤‘ì‹¬, -âˆ ~ +âˆ) <br>
     - ì •ê·œí™”: StandardScaler ëª¨ë“  ë³€ìˆ˜ì˜ ê°’ì„ í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸ ì •ê·œ ë¶„í¬ë¡œ ë³€í™˜ 
      ```python
      from sklearn.preprocessing import StandardScaler
      scaler_ss = StandardScaler()
      df_scaled_ss = scaler_ss.fit_transform(df_num)
      df_ss = pd.DataFrame(df_scaled_ss, columns= df_num.columns)
      df_ss
      #ì‹œê°í™” 
      fig, ax = plt.subplots( nrows= 1 , ncols=3, figsize=(14, 5))

      sns.histplot(x='salary', data=df_num, ax= ax[0])
      sns.histplot(x='salary', data=df_mm, ax= ax[1], color='green')
      sns.histplot(x='salary', data=df_ss, ax= ax[2], color='orange')

      ax[0].set_title('df salary histplot')
      ax[1].set_title('df_minmax salary histplot')
      ax[2].set_title('df_stan salary histplot')
      plt.show()
      ```
<br>

### <b>2. Get dummies</b>

```python
# í•´ë‹¹ columnë§Œ get_dummies
pd.get_dummies(df[['gender', 'blood_type', 'company', 'grades']],drop_first= True)  

#ì „ì²´ ë°ì´í„°ì— get_dummies
pd.get_dummies(df, columns =['gender', 'blood_type', 'company', 'grades'], drop_first= True)  
```

<br><br>

## <b>2. Feature Engineering</b>


1) filter (ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ë³€ìˆ˜ ì¶”ì¶œ)
- ì˜ˆì‹œ) ë…ë¦½ë³€ìˆ˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì¤‘ íšŒê·€ê³„ìˆ˜ê°€ ë†’ì€ 2ê°œ ì„ ì • (feature selection) í•˜ì‹œì˜¤
ë…ë¦½ë³€ìˆ˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì¤‘ tê²€ì • í†µê³„ëŸ‰ì˜ p-valueê°€ 0.05ì´í•˜ì¸ê²ƒì„ ì„ ì •í•˜ì‹œì˜¤
   ```python
   # p-value ê°’ 0.05 ë¯¸ë§Œ / coef ì ˆëŒ€ê°’ 0.5ì´ìƒì¸ ë³€ìˆ˜ë¥¼ ì„ íƒ
   model_ols.params.index[(model_ols.pvalues < 0.05)& (model_ols.params.abs() >= 0.5)]
   ```
2) wrapper
 - ì „ì§„ì„ íƒë²•, í›„ì§„ì œê±°ë²•, ë‹¨ê³„ì„ íƒë²•

3) embedded


<br><br>

## <b>3. Model</b>

> ## <b> Unsupervised Learning </b>
>  'ì˜ˆì¸¡'ë³´ë‹¤ëŠ” ì—†ì—ˆë˜ yë¥¼ ì°¾ëŠ” ê²ƒì— ì£¼ì•ˆì  <br>
>  ëª¨ë¸ì— fit ì‹œí‚¬ë•Œ yê°€ None


<br>

## <b>ê³„ì¸µì  êµ°ì§‘ë¶„ì„</b>
- :star: ì´ìƒì¹˜ ë° ë°ì´í„°ë³€ë™ì— ë¯¼ê°
- Nê°œì˜ ë°ì´í„°ê°€ ìˆì„ë•Œ Nê°œì˜ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘~ ìµœëŒ€ í•˜ë‚˜ì˜ êµ°ì§‘ì´ ë‚¨ì„ ë•Œê¹Œì§€ ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš´ ë‘ ìš”ì†Œë¥¼ êµ°ì§‘í™” ë¥¼ ë°˜ë³µ
- n_cluster ì„¤ì •ì‹œ n ê°œìˆ˜ê°€ ë  ë•Œê¹Œì§€ êµ°ì§‘í™”
- ì‚¬ì „ì— Kë¥¼ í• ë‹¹í•  í•„ìš”ê°€ ì—†ìŒ
- ê±°ë¦¬ê¸°ë°˜(ìœ ì‚¬ë„)ë¡œ ë¬¶ê¸°
   - ìµœë‹¨ì—°ê²°ë²•
   - ìµœì¥ì—°ê²°ë²•
   - í‰ê· ì—°ê²°ë²•
   - ì¤‘ì‹¬ì—°ê²°ë²•
   - ì™€ë“œì—°ê²°ë²• 
  <br>

- :star: ë°ì´í„°ê°œìˆ˜ê°€ ë§ì€ ê²½ìš° ì—°ì‚°ì— ë§ì€ ì‹œê°„ì´ ì†Œìš” <br>
  (5ì²œê°œ~ë§Œê°œë¥¼ ë„˜ê¸°ëŠ” ë°ì´í„°ì—ëŠ” ë¹„ê¶Œì¥)
- :star: ê³„ì¸µë„(Dendrogram) í™•ì¸ ê°€ëŠ¥: ë°ì´í„°ê°„ ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì‹í™”í•œ ë„í‘œ (modelì‚¬ìš©í•˜ì§€ì•Šê³  scipyì—ì„œ ìë£Œë¡œ ìŠ¤ìŠ¤ë¡œ ì—°ì‚°)
  
- [ë©”ì„œë“œ] 
  - sklearn-AgglomerativeClustering() <br>
      - n_clusters = ë¶„ë˜ êµ°ì§‘ê°œìˆ˜ ì„¤ì • <br>
      - Affinity = ë°ì´í„° ê°„ ê±°ë¦¬ê³„ì‚°ë°©ë²• <br>
      - Linkage = êµ°ì§‘ê°„ ìœ ì‚¬ë„ ë°©ë²• ì„¤ì • <br>
  - Spicy - dendrogram() 
  - Spicy - Linkage() =  ë°ì´í„° ê°„ ê±°ë¦¬ ê³„ì‚° ë° êµ°ì§‘ í˜•ì„±
- ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³µì‹ : ë‘ ê±°ë¦¬ diffë¥¼ ì œê³±í•˜ê³  ë”í•˜ì—¬ ë§ˆì§€ë§‰ì— ë£¨íŠ¸ë¥¼ ì”Œì›Œì¤Œ
  
```python
import pandas as pd
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt

df = pd.read_csv('./data/iris.csv')
df_x = df[['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width']]
model = AgglomerativeClustering(n_clusters = 3).fit(df_x) 
# y labelì´ ì—†ëŠ” ë¹„ì§€ë„í•™ìŠµ > ë…ë¦½ë³€ìˆ˜ë§Œ fit ì‹œí‚´
# (default) method = ward 
df['cluster'] = model.labels_
pd.crosstab(df['Species'], df['cluster'])

df.groupby('cluster').mean().reset_index()
# êµ°ì§‘ë³„ centroidêµ¬í•˜ê¸°


# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
link = linkage(df_x, method = 'ward') 
# link : clusterë§ ê²°ê³¼, ì²˜ìŒ ë‘ ê°œëŠ” í–‰ ë²ˆí˜¸, ê±°ë¦¬, í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë°ì´í„° ìˆ˜, ë¬¶ì–´ì§€ë©´ì„œ ìƒˆë¡œìš´ numbering ë¶€ì—¬ë¨
#(default) method = single ìœ„ ëª¨ë¸ê³¼ ì¼ì¹˜ ì‹œì¼œì¤Œ > wardë¡œ ë³€ê²½
dendrogram(link)
plt.show()

# ì‹œê°í™”ë¥¼ í†µí•´ insight ë°œêµ´ (black box ìœ ì¶”í•˜ê¸°)
# ì˜ˆì‹œ (ë…ë¦½,ì¢…ì† ê·¸ë˜í”„ì— hueë¡œ cluster ê²°ê³¼ê°’ ì…íˆê¸°)
fig, ax = plt.subplots( nrows= 1 , ncols=2, figsize=(14, 5))
sns.scatterplot(x='age', y='salary', data=basetable1, hue='company',  palette='Set1', ax= ax[0] )
sns.scatterplot(x='age', y='salary', data=basetable1, hue='cluster_hier',  palette='Set2', ax=ax[1] )

ax[0].set_title('category : company ')
ax[1].set_title('category : hierarchy cluster')
```

## <b>ë¹„ê³„ì¸µì  êµ°ì§‘ë¶„ì„</b>
- ì„ì˜ì˜ kì ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ê¹Œìš´ ê±°ë¦¬ì˜ ë°ì´í„°ë¥¼ ë¬¶ìŒ
- kë¥¼ í™•ì •í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ í•„ìš”
- ê²°ê³¼ë¥¼ ê³ ì •í•˜ê¸° ìœ„í•´ seedì„¤ì • í•„ìš” <br>
  ```í™œìš© : ê³ ê°êµ°ì§‘ì„ ë‚˜ëˆŒë•Œ Modelë¡œ ì˜ˆì¸¡í•˜ê³ , ê°€ì¥ ê´€ì‹¬ìˆëŠ” ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜ ì¢Œí‘œ ìœ„ì— labelê°’ì„ hueë¥¼ ë‘” ì‹œê°í™”ë¥¼ í†µí•´ ë‚˜ëˆˆ ê¸°ì¤€(black-box) ì„ ìœ ì¶”í•˜ë©° insightë¥¼ ê°€ì ¸ì˜¨ë‹¤ ```
  
```python
import pandas as pd
from sklearn.cluster import KMeans
cluster_1_2 = KMeans( n_clusters=3, random_state=123).fit(basetable_cluster_1) 
basetable1['cluster_kmean'] = cluster_1_2.labels_

basetable1.groupby('cluster_kmean').mean() # model.cluster_centers_ attribute ì™€ ê¸°ëŠ¥ë™ì¼, ë‹¨ í•´ë‹¹ ì½”ë“œëŠ” ì»¬ëŸ¼ì´ ì—†ì–´ ë³„ë„ dfì‘ì—…ì„ í•´ì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì—, groupbyë¥¼ ì§ì ‘ í•´ì£¼ëŠ” ê²ƒì´ ë” í¸ë¦¬

# Attribute í™•ì¸
cluster_1_2.inertia_



pd.crosstab( basetable1['cluster_hier'], basetable1['cluster_kmean']) #ê³„ì¸µì  modelê³¼ ë¹„êµ
pd.crosstab( basetable1['company'], basetable1['cluster_kmean'])

#ì‹œê°í™”
fig, ax = plt.subplots( nrows= 1 , ncols=3, figsize=(16, 5))
sns.scatterplot(x='age', y='salary', data=basetable1, hue='company',        palette='Set1', ax=ax[0] )
sns.scatterplot(x='age', y='salary', data=basetable1, hue='cluster_hier',   palette='Set1', ax=ax[1] )#ê³„ì¸µì  modelê³¼ ë¹„êµ
sns.scatterplot(x='age', y='salary', data=basetable1, hue='cluster_kmean',  palette='Set1', ax=ax[2] )

ax[0].set_title('category : company ')
ax[1].set_title('category : hierarchy cluster')
ax[2].set_title('category : kmeans cluster')
plt.show()


```

## <b>ëª¨ë¸í‰ê°€</b>
- Elbow score
  - kmeans inertia_ (ì‘ì§‘ë„)í™œìš©
  - Inertia ê°’, êµ°ì§‘í™”í›„ ê° ì¤‘ì‹¬ì ì—ì„œ êµ°ì§‘ì˜ ë°ì´íƒ€ê°„ ê±°ë¦¬ë¥¼ í•©ì‚°í•œê²ƒìœ¼ë¡œ ì‘ì§‘ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ = cluster ê°œìˆ˜ë¥¼ ë†’í ìˆ˜ë¡ inertiaëŠ” ì‘ì•„ì§, ê°’ì´ ì‘ì„ ìˆ˜ë¡ ì‘ì§‘ë„ê°€ ë†’ê²Œ êµ°ì§‘í™”ê°€ ì˜ë˜ì—ˆë‹¤ê³  í‰ê°€í•  ìˆ˜ ìˆìŒ
  - kê°€ ëª‡ì¼ë•Œ ì‘ì§‘ë„ê°€ ê°€ì¥ dramatic í•˜ê²Œ ê°ì†Œí•˜ëŠ”ì§€ í™•ì¸
   ```python
   # n_clusters=kë¥¼ 1ë¶€í„° 10ê¹Œì§€ ì ìš©

   inertias = []
   mapping = {}
   K = range(1, 10)

   for k in K:
      kmeanModel = KMeans(n_clusters=k).fit(basetable_cluster_1) 
      inertias.append(kmeanModel.inertia_)
      mapping[k] = kmeanModel.inertia_
      print('kê°’ ', k , '=>', kmeanModel.inertia_)

   # Elbow score ì‹œê°í™”
   plt.plot(np.arange(1, 10), inertias, 'bx-')
   plt.xlabel('Values of K')
   plt.ylabel('Inertia')
   plt.title('The Elbow Method using Inertia')
   plt.show()
   ```
- Silhouette score
    - silhouette_scoreê°€ 1ì— ê°€ê¹Œì›Œì•¼ Positive
    - ê° ë°ì´í„° í¬ì¸íŠ¸ì™€ ì£¼ìœ„ ë°ì´í„° í¬ì¸íŠ¸ë“¤ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°ì„ í†µí•´ ê°’ì„ êµ¬í•˜ë©°, êµ°ì§‘ ì•ˆì— ìˆëŠ” ë°ì´í„°ë“¤ì€ ì˜ ëª¨ì—¬ìˆëŠ”ì§€, êµ°ì§‘ë¼ë¦¬ëŠ” ì„œë¡œ ì˜ êµ¬ë¶„ë˜ëŠ”ì§€ í´ëŸ¬ìŠ¤í„°ë§ì„ í‰ê°€í•˜ëŠ” ì²™ë„ë¡œ í™œìš©
    - ê°’ì´ 0ì— ê°€ê¹Œìš´ ê²½ìš° : ë‘ êµ°ì§‘ ê°„ ê±°ë¦¬ê°€ ê±°ì˜ ë¹„ìŠ·í•œ ê²½ìš°   (ì˜ êµ¬ë¶„ë˜ì§€ ì•Šì€ ìƒíƒœ)
    - ê°’ì´ -1ì— ê°€ê¹Œìš´ ê²½ìš°ëŠ”, ë°ì´í„° í¬ì¸íŠ¸ iê°€ ì˜¤íˆë ¤ ì´ì›ƒ í´ëŸ¬ìŠ¤í„°ì— ë” ê°€ê¹Œìš´ ê²½ìš°ë¥¼ ì˜ë¯¸ (ì˜ëª» í• ë‹¹ëœ ìƒíƒœ) 
  - silhouette_score(data, ë¼ë²¨)
   ```python
   from sklearn.metrics import silhouette_score
   

   k_score = pd.DataFrame(columns =['k', 'score'])
   for i in np.arange(2, 7):
      model_clustering = KMeans( n_clusters=i, random_state=123).fit(basetable_cluster_1)
      a = silhouette_score( basetable_cluster_1,model_clustering.labels_)
      k = pd.DataFrame({'k':[i], 'score':[a]})
      k_score = pd.concat([k_score, k]).reset_index(drop=True)
      print("Kê°’ ", i, " silhouette score: ", a.round(3) )
   
   #ì‹œê°í™”
   sns.lineplot(x='k', y='score', data=k_score)
   plt.xticks([2, 3, 4, 5, 6])
   plt.show()
   ```





<br><br><br>
> ## <b> Supervised Learning </b>
<br>

## <b>ì„ í˜•íšŒê·€</b>
 - statsmodels vs sklearn ë¹„êµ 
 - statsmodelsëŠ” í†µê³„ê¸°ë°˜ ê´€ì   (summary í‘œ ë“± í†µê³„ìë£Œ ë³´ê¸° í¸í•¨) 
 - sklearnëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê´€ì 
 - ì…ë ¥ê°’ì˜ ì°¨ì´(statemodels olsì˜ ê²½ìš° formula ë¬¸ë²•ì´ ìˆìŒ / sklearnëŠ” fit() í™œìš©)
-  ols model ê³¼ sklearn ëª¨ë¸ ë‘ê°€ì§€ë¡œ ì ‘ê·¼ ê°€ëŠ¥
-  íšŒê·€ olsëŠ”  ì—°ì†í˜• ì¢…ì†ë³€ìˆ˜, ì—°ì†í˜• ë…ë¦½ë³€ìˆ˜
   ANOVA ols ëŠ” ì—°ì†í˜• ì¢…ì†ë³€ìˆ˜, ëª…ëª©í˜• ë…ë¦½ë³€ìˆ˜
-  ols.summary()ì—ì„œ fpvalueë¡œ ì„ í˜•ì„± ê²€ì¦
-  olsëŠ” í•™ìŠµí•œ ë°ì´í„°ë§Œ ë³„ë„ë¡œ ëª¨ë¸ì— ì í•©ì‹œí‚¤ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ ê±¸ëŸ¬ì„œ ì í•©í•¨
- ë°˜ë©´ sklearnì€ interceptë¡œ ì ˆí¸ ì í•© ì—¬ë¶€ ì„¤ì •ì´ ê°€ëŠ¥í•˜ì—¬ ê°•ë ¥í•œ ìµœì í™” ê°•ì ì„ ê°€ì§€ê³  ìˆë‹¤

<br>

### <b> ì„ í˜•íšŒê·€ ê°€ì • 4ê°€ì§€ ì„ í˜•ì„±, ì •ê·œì„±, ë“±ë¶„ì‚°, ë…ë¦½ì„±</b>
### <b>1. ì„ í˜•ì„±</b>
- F ê²€ì •ì˜ pvalueë¡œ í™•ì¸
- summary()ì˜ t ê²€ì • ê°’ë„ 'ì„ í˜•ì˜ ìœ íš¨ì„±'ì„ í™•ì¸í•˜ëŠ”ê²ƒì´ì§€ë§Œ, ì´ê²ƒì€ ë‘ì§‘ë‹¨ ê°„ ê²€ì¦ì§€í‘œë©°, xê°€ ë§ì€ ë‹¤ì¤‘íšŒê·€ì—ì„œëŠ” f-pvalueë¡œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.
- :star: ê·€ë¬´ê°€ì„¤ : ì„ í˜•ì„±ì´ ì—†ë‹¤
   ```python
   # ì„ í˜•íšŒê·€ ê·¸ë˜í”„, regplot: scatter plot, regression line, confidence bandë¥¼ í•œ ë²ˆì— ê·¸ë¦¬ëŠ” ê¸°ëŠ¥
   sns.regplot(x='salary', y='expenditure', data=df)
   sns.regplot(x=df['salary'], y=predict_ols)

   # F ê²€ì •ì˜ pvalueë¡œ í™•ì¸
   model_ols.f_pvalue 
   model_ols.f_pvalue < 0.05
   # ê²°ê³¼ : True (h1 ì§€ì§€ : ì„ í˜•ì„±ì´ ìˆë‹¤ê³  íŒë‹¨)
   ```
### <b>2. ì”ì°¨ì˜ ì •ê·œì„±</b>
- ì”ì°¨ ê·¸ë˜í”„ë¡œ í™•ì¸
- shapiro ì˜ ê²½ìš° pê°’ì´ 0.05 ì´ìƒì´ë©´ ì •ê·œì„± ë§Œì¡±í•œë‹¤
- :star: ê·€ë¬´ê°€ì„¤ : ì •ê·œì„±ì„ ë§Œì¡±í•œë‹¤ 

   ```python
   # ì”ì°¨ ê³„ì‚°
   residual = df['expenditure']-predict_ols

   # ì”ì°¨ ê·¸ë˜í”„ 1
   plt.scatter(df['salary'], residual)
   plt.show()
   # ì”ì°¨ ê·¸ë˜í”„ 2
   sns.distplot(residual)
   plt.show()

   # shapiro ì •ê·œì„± ê²€ì •, Ho: ì •ê·œì„±ì„ ê°€ì§„ë‹¤ (p-value > 0.05)
   from scipy.stats import shapiro
   shapiro(residual)[1] < 0.05
   # ê²°ê³¼í•´ì„: TRUE , 0.05 ë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°(= ì •ê·œì„±ì„ ë§Œì¡± ëª»í•¨)
   # residualì´ ê·¸ë£¹í™” ë˜ì–´ ìˆì–´ shapiro testì—ì„œ ì •ê·œì„±ì´ ì•ˆ ë‚˜ì˜´

   # ì‹œê°í™”
   import scipy.stats as stats
   stats.probplot(residual, plot=plt)
   plt.show()
   ```

### <b>3. ì”ì°¨ì˜ ë“±ë¶„ì‚°</b>
- ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨ì˜ ì‚°ì ë„ë¡œ íŒŒì•…
   ```python
   # ì”ì°¨ê·¸ë˜í”„ë¡œ í™•ì¸, Xê°€ ì»¤ì§ˆë•Œ ì”ì°¨ì˜ ê°„ê²©ì´ ë³€í•˜ë©´ ì•ˆë¨, ê°„ê²©ì´ ì¼ì •í•˜ë©´ ë“±ë¶„ì‚°ì„± ë§Œì¡±
   sns.regplot(x=predict_ols, y=residual)
   ```
### <b>4. ì”ì°¨ì˜ ë…ë¦½ì„±</b>
- ì”ì°¨ê°€ ë…ë¦½ì¸ì§€(ìê¸°ìƒê´€ì„±ì´ ìˆëŠ”ì§€) ê²€ì •
   ```python
   #perform Durbin-Watson test

   from statsmodels.stats.stattools import durbin_watson
   durbin_watson(model_ols.resid)

   # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰ì€ 0 ~ 4ì‚¬ì´ì˜ ê°’ì„ ê°–ì„ ìˆ˜ ìˆìŒ
   # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ì–‘ì˜ ìƒê´€ê´€ê³„
   # 4ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ìŒì˜ ìƒê´€ê´€ê³„
   # 2ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â†’ ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ì´ ì—†ìŒ
   ```
<br><br>

### <b> ë‹¨ìˆœì„ í˜•íšŒê·€</b>
### <b>statsmodels_ols</b>
```python
from statsmodels.formula.api import ols
model_ols = ols(formula = 'expenditure ~ salary', data= df).fit()

model_ols.summary()

model_ols.params #ì ˆí¸, íšŒê·€ê³„ìˆ˜
model_ols.resid #ì”ì°¨

# íšŒê·€ì‹ ë§Œë“¤ì–´ë³´ê¸°
def linear_ols(x):
    return (model_ols.params[1]* x + model_ols.params[0])
linear_ols(4720)

# predictë¡œ ì˜ˆì¸¡ê°’ í™•ì¸
predict_ols = model_ols.predict(df['salary'])

# ì‹œê°í™” 
fig, ax = plt.subplots( nrows= 1 , ncols=2, figsize=(14, 5))
sns.scatterplot(x=df['salary'], y=df['expenditure'], palette='Set1', ax= ax[0] )
sns.scatterplot(x=df['salary'], y=predict_ols, palette='Set2', ax=ax[1] )

ax[0].set_title('expenditure ')
ax[1].set_title('predict_ols')
plt.show()
```
### <b>statsmodels_OLS</b>
```python
# statsmodels.api
import statsmodels.api as sm

model_sm = sm.OLS(df_train1['salary'], sm.add_constant(df_train1[['age']])).fit()
model_sm

#sm.add_constant ìˆì–´ì•¼ ìƒìˆ˜í•­ì´ í¬í•¨ë¨
model_sm.predict(sm.add_constant(df_test1[['age']]))[:5]

# summary() #ìƒìˆ˜í•­í™•ì¸ê°€ëŠ¥
model_sm.summary()
```

### <b>sklearn</b></b>
```python
# LinearRegression í˜¸ì¶œ
from sklearn.linear_model import LinearRegression

# ëª¨ë¸ì„ íƒ, ë…ë¦½ë³€ìˆ˜(salary), ì¢…ì†ë³€ìˆ˜(expenditure) ì…ë ¥, fit 
model_lm = LinearRegression()
model_lm.fit(X = df[['salary']], y= df[['expenditure']]) 
#xë³€ìˆ˜ëŠ” dfí˜•íƒœê°€[[]] í•„ìˆ˜ì§€ë§Œ, yë³€ìˆ˜ì—ëŠ” []ë„ ê°€ëŠ¥ & []ë¡œ ì²˜ë¦¬í•´ì•¼ ê²°ê³¼ê°’ë„ []ë¡œ ë¦¬í„´ë˜ì–´ í›„ì²˜ë¦¬ì‹œ í¸ë¦¬
ê²°ê³¼ê°’ ê°€ê³µì— ìš©ì´
# íšŒê·€ê³„ìˆ˜ í™•ì¸
model_lm.coef_
# intercept_ í™•ì¸
model_lm.intercept_

# íšŒê·€ì‹ ë§Œë“¤ì–´ë³´ê¸°
def linear_lm(x):
    return (model_lm.coef_[0] * x) + model_lm.intercept_
# íšŒê·€ì‹ìœ¼ë¡œ ì˜ˆì¸¡
pred_lm = model_lm.predict(df[['salary']])

# ì„ í˜•íšŒê·€ ê·¸ë˜í”„
sns.regplot(x=df['salary'], y=pred_lm)
```
<br><br>

### <b>ë‹¤ì¤‘ì„ í˜•íšŒê·€</b>
### <b>ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ</b>
  - ë¶„ì‚°íŒ½ì°½ ê³„ìˆ˜(VIF)ê°€ 10ì´ìƒì´ë©´ ì œê±°
  - method: patsy- dmatrices() <br>
return_typeì¸ìì— dataframeìœ¼ë¡œ ì„¤ì •ì‹œ í›„ì²˜ë¦¬ ìš©ì´
- statsmodele-variance_inflation_factor() <br>
ë¶„ì‚°íŒ½ì°½ ê³„ìˆ˜ë¥¼ ì—°ì‚°ì„ ìœ„í•´ ë°˜ë³µë¬¸ ë˜ëŠ” list comprehension ì‚¬ìš©

```python
from patsy import dmatrices
df_sub = df.loc[:,'season':'casual']
formula ='casual ~ '+ '+'.join(df_sub.columns[:-1])
y, X = dmatrices(formula , data = df_sub, return_type = 'dataframe')

#ë°˜ë³µë¬¸ ëŒë¦¬ë©´ì„œ ë³€ìˆ˜ë³„ í•˜ë‚˜ì”© ë§¤ì¹­í•˜ì—¬ ìƒê´€ê³„ìˆ˜ êµ¬í•¨
from statsmodels.stats.outliers_influence import variance_inflation_factor as via
df_vif = pd.DataFrame()
df_vif['colname'] = X.columns
df_vif['VIF'] = [vif(X.values, i) for i in range(X.shape[1])]
df_vif
#ê²°ê³¼ê°’ì—ì„œ interceptëŠ” ì ˆí¸í•­ì´ë‹ˆê¹Œ ì‹ ê²½ì•ˆì¨ë„ë¨
```
<br><br>

## <b> ë¡œì§€ìŠ¤í‹± íšŒê·€</b>
-  ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥¼ ê²½ìš° ì‚¬ìš©
-  (pred>0.9) + 0) #ê¸°ë³¸ê°’ 0.5 ì™¸ threshold ì„¤ì •ì‹œ ìœ ìš©
### <b>statsmodels_logit</b>
```python
import pandas as pd
import numpy as np
from statsmodels.api import Logit

df['is_setosa'] = (df['Species'] =='setosa') + 0
model = Logit(df['is_setosa'], df.iloc[:,:2]).fit()
model   #í†µê³„ëª¨ë¸ : (ì¢…ì†, ë…ë¦½) ìˆœì„œ ì£¼ì˜

pred = model.predict(df.iloc[:3,:2])
pred_class = (pred > 0.5) +0 
#í†µê³„ëª¨ë¸ì€ ê²°ê³¼ê°’ì´ proba í™•ë¥ ë¡œ ì‚°ì¶œë˜ê¸° ë•Œë¬¸ì—, thresholdê°’ ì§€ì •í•˜ì—¬ 2ì§„ ë¶„ë¥˜ ìˆ˜ê¸°ë¡œ ì§„í–‰í•´ì¤˜ì•¼í•¨. ()ì•ˆì˜ ì¡°ê±´ì— í•´ë‹¹ë˜ëŠ” ê²ƒì´ 1 


#ìŠ¹ì‚°ë¹„ êµ¬í•˜ê¸° (íšŒê·€ê³„ìˆ˜ë¥¼ expí•´ì£¼ë©´ ëœë‹¤.)
np.exp(model_lo2.params)
```
### <b>statsmodels_formula_logit</b>
```python
from statsmodels.formula.api import logit

# formula ìƒì„±
form = 'car_type ~ ' + ' + '.join(df_hk_0.drop('car_type',axis =1).columns)
form  #í†µê³„ëª¨ë¸ : (ì¢…ì†, ë…ë¦½) ìˆœì„œ ì£¼ì˜

model_logit = logit(form, data = df_hk1_0).fit()
model_logit.summary()
```

### <b>statsmodels</b>

```python
import statsmodels.api as sm

model_smLogit = sm.Logit(endog = df_hk1_0['car_type'], exog = sm.add_constant(df_hk1_0.drop('car_type', axis=1))).fit()
# í†µê³„ëª¨ë¸ : (ì¢…ì†, ë…ë¦½) ìˆœì„œ ì£¼ì˜
# sm.add_constant interceptìœ¼ë¡œ ìƒìˆ˜í•­ í¬í•¨ì‹œí‚¤ê¸°

model_smLogit.summary()
```

### <b>sklearn</b>
- solver ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„¤ì • ê°€ëŠ¥ (ìµœì ê°’ì„ ì‚°ì¶œí• ë•Œ ë§ì´ ì‚¬ìš©)
```python
from sklearn.linear_model import LogisticRegression

model_lg = LogisticRegression(C=100000 ,solver='newton-cg')
# C=100000: ì •ê·œí™”(L1, L2ê·œì œ) ê°•ë„ì˜ ì—­ìˆ˜; ì–‘ì˜ ì‹¤ìˆ˜, ê°’ì´ ì‘ì„ìˆ˜ë¡ ë” ê°•ë ¥í•œ ì •ê·œí™” ì§€ì •

model_lg.fit(df.iloc[:,:2],df['is_setosa'])

pred = model_lg.predict_proba(df.iloc[:,:2])
pred = pred[:,1]
roc_auc_score(df['is_setosa'], pred) # ì‹¤ì¸¡ê°’, ì˜ˆì¸¡ê°’ìˆœ
accuracy_score(df['is_setosa'], (pred>0.9) + 0) 
```

<br><br>
## <b>ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ</b>
-  ì‚¬ì „í™•ë¥  ë° ì¶”ê°€ì •ë³´ë¥¼ ê¸°ê°„ìœ¼ë¡œ ì‚¬í›„í™•ë¥ ì„ ì¶”ë¡ í•˜ëŠ” í†µê³„ê¸°ë²• 'ë² ì´ì¦ˆ ì¶”ì •' ê¸°ë°˜ ë¶„ë¥˜
-  ì¢…ì†ë³€ìˆ˜ ê° ë²”ì£¼ì˜ ë“±ì¥ë¹ˆë„ì¸ 'ì‚¬ì „í™•ë¥ ' ì„¤ì •ì´ ì¤‘ìš”
-  ê° ë°ì´í„°ì˜ ì‚¬ì „ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬í›„í™•ë¥  ê³„ì‚°
-  ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì†í˜• ë³€ìˆ˜ì¼ ë•Œ, Gaussian Naive Bayes (ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ)
-  ì¢…ì†ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë³€ìˆ˜ì¼ ë•Œ, Multinomial Naive Bayes (ë‹¤í•­ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ) 
  
    
### <b>sklearn</b>
```python
import pandas as pd
from sklearn.naive_bayes import GaussianNB

df =pd.read_csv('./data/iris.csv')
df['is_setosa'] = (df['Species']=='setosa') + 0

# ì‚¬ì „í™•ë¥ (ë¹„ìœ¨) êµ¬í•˜ê¸°
df['is_setosa'].value_counts(normalize = True)


model_g = GaussianNB()
model_g.fit(df.iloc[:,:4], df['is_setosa'])

# ì‚¬ì „í™•ë¥  ë©”ì„œë“œ í™œìš©í•˜ì—¬ êµ¬í•˜ê¸°
model_g.class_prior_

# ê³„ìˆ˜
model_g.theta_

pred= model_g.predict_proba(df.iloc[:,:4])
pred
```

<br><br>
## <b>Decision Tree</b>
- ë¹„ëª¨ìˆ˜ì  ê¸°ë²•
- ì „ì œ ê°€ì •(ì •ê·œì„±, ë…ë¦½ì„± ë“±)ì´ ì—†ì–´ë„ë¨
- ìˆ˜ì¹˜í˜•, ë²”ì£¼í˜• ë°ì´í„° ëª¨ë‘ ì ìš© ê°€ëŠ¥
- ë‹¨, ì „ì—­ ìµœì í™”ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ ìˆìŒ
- ê³¼ì í•©ì´ ì‰½ê²Œ ë°œìƒ
- ìë£Œ ê°€ê³µì´ ê±°ì˜ í•„ìš” ì—†ìŒ
- max_depth : íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (default = None
â†’ ì™„ë²½í•˜ê²Œ í´ë˜ìŠ¤ ê°’ì´ ê²°ì •ë  ë•Œ ê¹Œì§€ ë¶„í•  <br>
ë˜ëŠ” ë°ì´í„° ê°œìˆ˜ê°€ min_samples_splitë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ ë¶„í• )
â†’ depthë¥¼ ë†’íìˆ˜ë¡ í”¼íŒ… (êµ¬ê°„) ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚¨ <br> 
(í”¼íŒ…ì€ ì¼ì • êµ¬ê°„ë§ˆë‹¤ í‰ê· ê°’ìœ¼ë¡œ í•˜ê²Œ ë˜ì–´ìˆë‹¤) <br>
(ì‹œê°í™”ì‹œ ê³„ë‹¨í˜•ìœ¼ë¡œ í‘œí˜„ë¨)<br>
  
- min_samples_split : ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„°ìˆ˜ â†’ ê³¼ì í•©ì„ ì œì–´í•˜ëŠ”ë° ì‚¬ìš©
(default = 2 â†’ ì‘ê²Œ ì„¤ì •í•  ìˆ˜ë¡ ë¶„í•  ë…¸ë“œê°€ ë§ì•„ì ¸ ê³¼ì í•© ê°€ëŠ¥ì„± ì¦ê°€)

```python
# DecisionTreeClassifier í˜¸ì¶œ
from sklearn.tree import DecisionTreeClassifier

model_dt = DecisionTreeClassifier(max_depth = 6, min_samples_split= 5, random_state =1234)

model_dt.fit(train_df_hk_3.drop('car_type',axis=1),
             train_df_hk_3['car_type'])

# feature_importances_  ì¢…ì†ë³€ìˆ˜(car_typeì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì •ë„) -> feature_importances_ë¡œ ë¶„ê¸° í•œë‹¤
model_imp = pd.DataFrame()
model_imp['feature']= model_dt.feature_names_in_
model_imp['feature_importance']= model_dt.feature_importances_
model_imp.sort_values(by ='feature_importance',ascending=False)

# plot_tree ì‹œê°í™”
from sklearn.tree import plot_tree
plt.figure(figsize=(35,10))
a = plot_tree(model_dt, 
              feature_names= model_dt.feature_names_in_,          
              # feature_names display
              class_names =  train_df_hk_3['car_type'].unique(), 
              # class_names display
              filled=True,                                       
              # color
              rounded=True,
              max_depth=4,                                      # display max_depth= 4
              fontsize=14)
```

<br>

## <b>KNN</b>
- ë¹„ëª¨ìˆ˜ ë°©ì‹
- ë¶„ë¥˜/íšŒê·€ ëª¨ë‘ ê°€ëŠ¥
- ë™ë¥ ì„ ë§‰ê¸° ìœ„í•´ kê°’ì€ ë³´í†µ í™€ìˆ˜ë¡œ ì •í•¨ (1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ê³¼ì í•©)
- 'ê±°ë¦¬'ê°œë…ì„ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, í‘œì¤€í™”/ì •ê·œí™” ì‚¬ìš© í•„ìˆ˜
- ê±°ë¦¬ê¸°ì¤€ì€ ì¼ë°˜ì ìœ¼ë¡œ ë§¨í—ˆí„´ê±°ë¦¬ í˜¹ì€ ìœ í´ë¦¬ì•ˆ ê±°ë¦¬ ì‚¬ìš©
```python
from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor

#Classifier
df2 = pd.read_csv('./data/diabetes.csv')
df2['is_preg'] = (df2['Pregnancies'] > 0) + 0

X = df2[['Pregnancies', 'Glucose', 'BloodPressure','Insulin','BMI']]
Y = df2[['is_preg']]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 123, test_size = 0.2)

k = [3,5,10,20]
score = []

for i in k:
    model_kc = KNeighborsClassifier(n_neighbors = i)
    model_kc.fit(x_train, y_train)
    pred_g = model_kc.predict(x_test)
    score.append(accuracy_score(y_test,pred_g))
    print('kê°€',i,'ì¼ë•Œ :',accuracy_score(y_test,pred_g))

result= pd.DataFrame()
result['k'] = k
result['score'] = score

result

#forë¬¸ì„ í™œìš©í•˜ì—¬ K íŒŒë¼ë¯¸í„°ê°’ì„ ì—¬ëŸ¬ê°œ ì¤˜ë³´ê³ , f1score, accuracy_score ë“± ì ìˆ˜ê°€ ë†’ì€ ê°’ì„ ì°¾ëŠ”ë‹¤. ë˜ëŠ” random_search / grid_search í™œìš©
```  





<br><br><br><br>


## <b>ëª¨ë¸í‰ê°€</b>
<b>1. ìˆ˜ì¹˜í˜•</b>
- R-square(ê²°ì •ê³„ìˆ˜)
- MAE
   ```python
   from sklearn.metrics import mean_absolute_error
   mean_absolute_error(yì‹¤ì¸¡ê°’, yì˜ˆì¸¡ê°’)
   ```
- MSE
   ```python
   from sklearn.metrics import mean_squared_error
   mean_squared_error(yì‹¤ì¸¡ê°’, yì˜ˆì¸¡ê°’)
   ```
- RMSE
   ```python
   mean_squared_error(yì‹¤ì¸¡ê°’, yì˜ˆì¸¡ê°’) ** 0.5
   ```

<b>2. ë²”ì£¼í˜•</b>

- f1-score (ì¡°í™”í‰ê· )<br>
(ì‘ì€ ê°’ ìª½ì— Advantageë¥¼ ì£¼ì–´ í‰ê· ì„ ì´ ë” ê°€ê¹Œì›Œì§) <br>
f1ì€ ì–´ëŠì‹œì ê¹Œì§€ ìƒìŠ¹í–ˆë‹¤ê°€ í•˜ê°•í•˜ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆì–´, ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì§€ì ì´ recallê³¼ precisionì˜ ì ì •í•˜ê²Œ ì¡°í™”ë¡œìš´ ê°’ì´ë‹¤.<br>
ê³µì‹ : 2* ( (Recall * Precision )/ Recall + Precision)
   ```python
   from sklearn.metrics import f1_score
   ```
- ì •í™•ë„
   ```python
   from sklearn.metrics import accuracy_score
   ```
- ì •ë°€ë„
  ```python
  from sklearn.metrics import precision_score
  ```
- ì¬í˜„ìœ¨
  ```python
  from sklearn.metrics import recall_score
  ```
- AUC 
  ```python
  from sklearn.metrics import roc_auc_score
  ```
- ì´ ìš”ì•½í‘œ
  ```python
  from sklearn.metrics import classification_report
  #print()ë¡œ ì¶œë ¥
  ```



<b> [TIP] Sample data ë§Œë“¤ê¸°<b> <br>
df.pd.DataFrame[[0],]

<br><br><br><br><br>
---------

<b>ì‹¤ê¸° ë¬¸ì œí’€ì´</b>
```python
import pandas as pd
import numpy as np
df = pd.read_csv('./data/DS_Sample_3.csv',encoding ='utf-8')

movie3 = df.copy()
movie3['AirDate'] = pd.to_datetime(movie3['AirDate'])

movie3['group'] = np.where(movie3['AirDate'].dt.year.isin([2005,2006,2007]),'A',
         np.where(movie3['AirDate'].dt.year.isin([2008,2009,2010]),'B','C'))
#between(2005,2007)ë„ ë¨

movie3['success'] = movie3['Rating'] * movie3['Num_Votes'] 

movie3.groupby('DirectedBy')['success'].mean().max().astype(int)

from statsmodels.stats.anova import anova_lm 
from statsmodels.formula.api import ols

model = ols(formula = 'Num_Votes~ group', data= movie3).fit()
anova_lm(model)['F'][0] + anova_lm(model)['df'][0]
```