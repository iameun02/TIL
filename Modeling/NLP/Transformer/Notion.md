# Similarity
단어나 문장간 유사도 비교 <br>
단어나 문장을 벡터로 변환 후 유사도 비교
 - Euclidean Distance
    - 벡터간의 거리를 계산하여 유사도를 측정
    - 거리가 짧을 수록 유사도 높음
 - Cosine
    - 두 벡터간의 사이갓을 계산하여 유사도를 측정
    - 사잇각이 작을수록 유사도가 높음
    - 벡터의 크기가 아닌 방향성 기반
<br> <br> <br>

# Embedding
단어나 문장을 벡터로 변환후 벡터공간으로 끼워넣는 의미 <br>
Word Embedding은 단어를 희소표현이 아닌 밀집표현으로 변환
- 희소표현
  - 대부분 값이 0으로 표현되어, 공간낭비 발생 및 단어의 의미를 담지못함
- 밀집표현
  - 지정된 차원의 밀집벡터로, 실수값을 사용하여 표현하고 문맥의 의미를 담고 있음 <br> <br>

|    |One-hot Vector|Embedding Vector|
|:---:|:---:|:---:|
|차원|고차원(단어 집합의 크기|저차원(사용자지정)|
|구조|희소벡터|밀집벡터|
|표현방법|확정적|데이터로부터학습|
|값의타입|0과1|실수|

# 분류헤드튜닝 vs 언어모델미세튜닝
분류 헤드 튜닝은 사전 학습된 언어 모델의 상단에 새로운 분류 헤드를 추가하고, 이 헤드를 새로운 분류 작업에 맞게 훈련시키는 과정을 의미합니다. 이 방법은 사전 학습된 언어 모델의 일부 파라미터만 업데이트하므로 학습 시간이 상대적으로 짧고, 라벨이 있는 분류 작업에 특히 유용합니다. 예를 들어, 이미지 분류 작업을 위해 사전 학습된 컨볼루션 신경망에 분류 헤드를 추가하고, 해당 헤드를 훈련하여 이미지를 다양한 클래스로 분류할 수 있습니다.

반면, 언어 모델 미세 튜닝은 사전 학습된 언어 모델 전체를 새로운 데이터셋에 맞게 추가적으로 훈련시키는 과정을 의미합니다. 이 방법은 사전 학습된 모델의 모든 파라미터를 업데이트하므로 더 많은 계산 리소스와 학습 시간이 필요합니다. 언어 모델 미세 튜닝은 특정한 자연어 처리 작업을 수행하기 위해 모델의 언어 이해 능력을 개선하는 데 사용됩니다. 예를 들어, 챗봇이나 기계 번역 작업에 사용되는 언어 모델을 특정 도메인이나 언어에 특화되도록 훈련시킬 수 있습니다.

요약하면, 분류 헤드 튜닝은 사전 학습된 모델의 일부 파라미터를 업데이트하여 분류 작업에 맞게 조정하는 반면, 언어 모델 미세 튜닝은 사전 학습된 모델 전체를 새로운 데이터셋에 맞게 업데이트하여 특정 자연어 처리 작업에 더 잘 맞도록 개선하는 것입니다.
<br> <br> <br>

# 파라미터 vs 하이퍼파라미터
파라미터는 모델 내부에서 학습되는 가중치 값입니다. 모델은 주어진 데이터에 맞게 파라미터를 조정하여 특정 작업을 수행하는 능력을 학습합니다. 파라미터는 학습 알고리즘에 의해 업데이트되며, 데이터와 손실 함수에 기초하여 계산됩니다. 예를 들어, 신경망의 가중치와 편향은 학습 중에 업데이트되는 파라미터입니다.

반면에 하이퍼파라미터는 모델을 학습하기 전에 사전에 설정되는 매개 변수입니다. 하이퍼파라미터는 모델의 학습 동작을 제어하고 조정하는 역할을 합니다. 학습률, 배치 크기, 에폭 수, 정규화 강도 등이 하이퍼파라미터에 해당합니다. 하이퍼파라미터의 값은 사람이 수동으로 설정하며, 모델의 학습 과정에 직접적으로 영향을 줍니다. 하이퍼파라미터를 조정하면 모델의 학습 속도, 일반화 능력, 오버피팅 등에 영향을 줄 수 있습니다.

요약하면, 파라미터는 모델 내부에서 학습되는 가중치 값이며, 학습 데이터에 맞게 조정됩니다. 하이퍼파라미터는 모델의 학습 동작을 제어하는 매개 변수로, 학습 전에 수동으로 설정됩니다.

# 모델에 입력되는 시퀀스 길이 통일의 필요 여부

패딩(Padding): 시퀀스의 길이가 다른 경우, 패딩을 사용하여 모든 시퀀스를 동일한 길이로 맞출 수 있습니다. 패딩은 일반적으로 특정 값을 사용하여 시퀀스를 채웁니다. 가장 일반적으로는 0을 사용하며, 패딩된 부분은 모델에 영향을 주지 않도록 처리됩니다. 이를 통해 모델은 일정한 길이의 입력을 처리할 수 있습니다.

자르기(Truncation): 반대로, 입력 시퀀스가 너무 긴 경우 일부를 자를 수 있습니다. 자르기는 시퀀스를 미리 정의된 최대 길이로 자르는 것을 의미합니다. 긴 시퀀스의 일부 정보가 손실될 수 있으나, 모델이 일정한 크기의 입력을 처리할 수 있게 됩니다.

동적 길이 입력: 일부 모델은 동적 길이 입력을 처리할 수 있는 능력을 가질 수 있습니다. 이러한 모델은 입력의 길이에 따라 자동으로 적응하고, 각 시퀀스를 독립적으로 처리합니다. 예를 들어, Transformer 모델은 입력 시퀀스의 길이에 제한이 없습니다.

배치 처리: 모델은 보통 여러 개의 입력 시퀀스를 동시에 처리할 수 있습니다. 따라서, 배치로 여러 시퀀스를 그룹화하여 한 번에 모델에 주입할 수 있습니다. 각 배치의 시퀀스는 길이가 다를 수 있습니다. 이를 위해 패딩 또는 자르기가 배치 내에서 적용될 수 있습니다.

따라서, 모델에 입력되는 시퀀스의 길이가 항상 같아야 하는 것은 아니며, 다양한 방법을 사용하여 가변 길이의 시퀀스를 처리할 수 있습니다. 어떤 방법을 선택할지는 작업과 모델의 특성에 따라 다를 수 있습니다.

# 학습 원리

모델 아키텍처 정의: 모델 아키텍처는 모델의 구조와 연산을 정의합니다. 예를 들어, 신경망 모델에서는 층(layer)과 활성화 함수(activation function)를 정의하고, 이를 통해 입력 데이터를 처리하는 방법을 결정합니다.
(*참고 : 언어 모델 아키텍처 선택: 언어 모델의 아키텍처를 선택합니다. 이는 모델이 어떤 유형의 신경망 구조를 사용할지 결정하는 단계입니다. 예를 들어, 순환 신경망 (Recurrent Neural Network, RNN)이나 변형된 변종인 LSTM (Long Short-Term Memory) 또는 Transformer 등의 아키텍처를 선택할 수 있습니다.)

가중치 초기화: 모델의 가중치는 처음에 무작위로 초기화됩니다. 초기화는 모델의 학습을 시작하는 지점을 정의하는 역할을 합니다.

순방향 전파(Forward Propagation): 입력 데이터는 모델의 순방향 전파 과정을 통해 처리됩니다. 입력 데이터(텐서)가 모델의 첫 번째 층에 주입되고, 이후 각 층에서 연산이 수행됩니다. 층은 입력 데이터를 가중치와 결합하여 활성화 함수를 통과시킵니다. 결과적으로 출력값이 생성됩니다.

손실 함수 계산: 모델의 출력값과 정답(레이블)을 비교하여 손실 함수를 계산합니다. 손실 함수는 모델의 예측과 실제 값 간의 차이를 측정합니다. 예를 들어, 분류 작업에서는 교차 엔트로피 손실 함수를 사용할 수 있습니다.

역전파(Backpropagation): 역전파 알고리즘을 사용하여 손실 함수의 그래디언트(기울기)를 계산합니다. 역전파는 오류를 거꾸로 전파하면서 각 층의 가중치에 대한 그래디언트를 계산하는 과정입니다. 이를 통해 각 가중치가 손실 함수의 값을 최소화하는 방향으로 업데이트될 수 있습니다.

가중치 업데이트: 계산된 그래디언트를 사용하여 가중치를 업데이트합니다. 일반적으로 경사하강법(Gradient Descent) 알고리즘이 사용됩니다. 경사하강법은 현재 가중치와 그래디언트의 곱을 학습률(learning rate)로 조절하여 가중치를 조금씩 업데이트하는 방법입니다. 이를 반복하면서 모델의 가중치가 학습 데이터에 맞게 조정됩니다.

반복: 위의 과정을 반복하여 모델의 성능을 향상시킵니다. 학습 데이터를 여러 번 반복하여 모델이 데이터의 패턴을 학습할 수 있도록 합니다. 각 반복을 에포크(epoch)라고 부릅니다.

이러한 과정을 통해 모델은 입력 데이터를 순방향으로 전파하고, 손실 함수를 통해 예측과 실제 값을 비교합니다. 그 후 역전파를 통해 그래디언트를 계산하고, 가중치를 업데이트하여 모델의 성능을 향상시킵니다. 이러한 학습 과정은 반복적으로 진행되면서 모델이 주어진 작업에 대해 최적화됩니다.