# Tokenizer


1. 전통적인 통계 방식 : 단어의 빈도수로 Bow(Bag of Words) > 순서를 유지못함, N-gram사용시 피처수가 기하급수적으로 증가 <br>
--> TF-IDF(전체문서에서 공통으로 사용되는 단어는 낮음, 특정문서에서 자주사용되는 단어는 중요도 가 높아 높음) 사용 <br>
Scikit-Learn의 TfidfVectorizer Method 사용 <br>
2. 현재 방식 : Vocabrary 사전화 하여 One-hot encoding (나머지, unk)
> 하지만 단어를  단순 index  번호에 따라 one-hot encoding 으로 vetorize하므로 단어간의 유사성을 파악하지 못함
> 워드 임배딩 개념 등장 : 1) 단어/문장 간 관련도 계산 2) 의미적/문법적 정보 함축/ 3) 전이학습 가능ㅇ
>3. 워드임배딩 : 숫자화된 단어의 나열로 부터 
연관성있는 단어들을 군집화하여 다차원 공간에 백터로 표시
즉, 단어나 문장을 벡터 스페이스에 끼워넣음 : 임베딩

기쁨 슬픔 어려움


2013년 Word2Vec : 구글에서 개발한 워드 임베딩 방법
차원(피처)를 나누는기준은 블랙박스이며
뉴럴 네트워크가 알아서 손실함수의 손실을 줄이며 학습하며 가중치를 업데이트 해가는것 
'여기까진 머신러닝 개념'

